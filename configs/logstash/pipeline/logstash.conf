# Logstash Pipeline Configuration
# AIOps-Platform 日志处理管道配置
# 版本: 1.0.0
# 功能: 处理来自各种源的日志数据，进行解析、过滤、增强后发送到Elasticsearch

# ===========================================
# 输入配置 (Input)
# ===========================================
input {
  # Beats输入 - 接收来自Filebeat, Metricbeat等的数据
  beats {
    port => 5044
    host => "0.0.0.0"
    client_inactivity_timeout => 60
    include_codec_tag => false
  }

  # Syslog输入 - 接收系统日志
  syslog {
    port => 5514
    host => "0.0.0.0"
    codec => "json"
    tags => ["syslog"]
  }

  # TCP输入 - 接收应用程序日志
  tcp {
    port => 5000
    host => "0.0.0.0"
    codec => "json_lines"
    tags => ["tcp"]
  }

  # UDP输入 - 接收网络设备日志
  udp {
    port => 5001
    host => "0.0.0.0"
    codec => "json"
    tags => ["udp"]
  }

  # HTTP输入 - 接收HTTP POST的日志
  http {
    port => 8080
    host => "0.0.0.0"
    codec => "json"
    tags => ["http"]
  }

  # Redis输入 - 从Redis队列读取日志
  redis {
    host => "redis"
    port => 6379
    key => "logstash"
    data_type => "list"
    codec => "json"
    tags => ["redis"]
  }

  # Kafka输入 - 从Kafka消费日志消息
  kafka {
    bootstrap_servers => "kafka:9092"
    topics => ["logs", "metrics", "traces"]
    group_id => "logstash-consumer-group"
    consumer_threads => 4
    codec => "json"
    tags => ["kafka"]
  }

  # 文件输入 - 监控日志文件
  file {
    path => ["/var/log/*.log", "/var/log/**/*.log"]
    start_position => "beginning"
    sincedb_path => "/usr/share/logstash/data/sincedb"
    codec => "multiline" {
      pattern => "^%{TIMESTAMP_ISO8601}"
      negate => true
      what => "previous"
    }
    tags => ["file"]
  }

  # JDBC输入 - 从数据库读取数据
  jdbc {
    jdbc_driver_library => "/usr/share/logstash/logstash-core/lib/jars/postgresql.jar"
    jdbc_driver_class => "org.postgresql.Driver"
    jdbc_connection_string => "jdbc:postgresql://postgres:5432/aiops"
    jdbc_user => "${POSTGRES_USER:aiops}"
    jdbc_password => "${POSTGRES_PASSWORD:aiops123}"
    schedule => "*/5 * * * *"
    statement => "SELECT * FROM audit_logs WHERE created_at > :sql_last_value ORDER BY created_at"
    use_column_value => true
    tracking_column => "created_at"
    tracking_column_type => "timestamp"
    tags => ["database"]
  }
}

# ===========================================
# 过滤配置 (Filter)
# ===========================================
filter {
  # 添加通用字段
  mutate {
    add_field => {
      "[@metadata][pipeline]" => "aiops-main"
      "[@metadata][version]" => "1.0.0"
      "platform" => "aiops"
    }
  }

  # 处理Docker容器日志
  if [container] {
    # 解析容器名称
    grok {
      match => { "[container][name]" => "/(?<container_service>[^_]+)" }
    }
    
    # 添加容器标签
    mutate {
      add_field => { "service_type" => "container" }
      add_tag => ["containerized"]
    }
  }

  # 处理Kubernetes日志
  if [kubernetes] {
    # 解析Kubernetes元数据
    mutate {
      add_field => {
        "k8s_namespace" => "%{[kubernetes][namespace]}"
        "k8s_pod" => "%{[kubernetes][pod][name]}"
        "k8s_container" => "%{[kubernetes][container][name]}"
      }
      add_tag => ["kubernetes"]
    }
  }

  # 处理应用程序日志
  if "application" in [tags] {
    # 解析应用日志格式
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{LOGLEVEL:log_level}\] %{DATA:logger_name} - %{GREEDYDATA:log_message}"
      }
    }
    
    # 解析时间戳
    date {
      match => [ "timestamp", "ISO8601" ]
      target => "@timestamp"
    }
    
    # 标准化日志级别
    translate {
      field => "log_level"
      destination => "severity"
      dictionary => {
        "TRACE" => "1"
        "DEBUG" => "2"
        "INFO" => "3"
        "WARN" => "4"
        "ERROR" => "5"
        "FATAL" => "6"
      }
      fallback => "3"
    }
  }

  # 处理Web访问日志
  if "nginx" in [tags] or "apache" in [tags] {
    # 解析访问日志
    grok {
      match => {
        "message" => "%{COMBINEDAPACHELOG}"
      }
    }
    
    # 解析用户代理
    useragent {
      source => "agent"
      target => "user_agent"
    }
    
    # GeoIP解析
    geoip {
      source => "clientip"
      target => "geoip"
    }
    
    # 计算响应时间分类
    if [response_time] {
      ruby {
        code => "
          response_time = event.get('response_time').to_f
          if response_time < 0.1
            event.set('response_category', 'fast')
          elsif response_time < 1.0
            event.set('response_category', 'normal')
          elsif response_time < 5.0
            event.set('response_category', 'slow')
          else
            event.set('response_category', 'very_slow')
          end
        "
      }
    }
  }

  # 处理系统日志
  if "syslog" in [tags] {
    # 解析syslog格式
    grok {
      match => {
        "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{IPORHOST:syslog_server} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}"
      }
    }
    
    # 解析时间戳
    date {
      match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
    }
  }

  # 处理错误日志
  if [log_level] == "ERROR" or [log_level] == "FATAL" {
    # 提取错误信息
    grok {
      match => {
        "log_message" => "(?<error_type>[A-Za-z]+Exception|Error): %{GREEDYDATA:error_details}"
      }
      tag_on_failure => ["_grokparsefailure_error"]
    }
    
    # 添加错误标签
    mutate {
      add_tag => ["error", "alert"]
      add_field => { "alert_priority" => "high" }
    }
  }

  # 处理性能指标
  if "metrics" in [tags] {
    # 解析指标数据
    if [metric_name] {
      mutate {
        add_field => { "metric_type" => "performance" }
      }
      
      # 计算指标阈值告警
      if [metric_name] == "cpu_usage" and [metric_value] {
        ruby {
          code => "
            cpu_usage = event.get('metric_value').to_f
            if cpu_usage > 90
              event.set('alert_level', 'critical')
              event.tag('cpu_alert')
            elsif cpu_usage > 80
              event.set('alert_level', 'warning')
              event.tag('cpu_warning')
            end
          "
        }
      }
      
      if [metric_name] == "memory_usage" and [metric_value] {
        ruby {
          code => "
            memory_usage = event.get('metric_value').to_f
            if memory_usage > 95
              event.set('alert_level', 'critical')
              event.tag('memory_alert')
            elsif memory_usage > 85
              event.set('alert_level', 'warning')
              event.tag('memory_warning')
            end
          "
        }
      }
    }
  }

  # 处理安全日志
  if "security" in [tags] {
    # 检测可疑活动
    if [message] =~ /failed login|authentication failed|unauthorized access/ {
      mutate {
        add_tag => ["security_event", "failed_auth"]
        add_field => { "security_category" => "authentication" }
      }
    }
    
    if [message] =~ /sql injection|xss|csrf/ {
      mutate {
        add_tag => ["security_event", "attack_attempt"]
        add_field => { "security_category" => "web_attack" }
      }
    }
  }

  # 数据清理和标准化
  mutate {
    # 移除不需要的字段
    remove_field => ["beat", "offset", "prospector", "input"]
    
    # 标准化主机名
    lowercase => ["host"]
    
    # 添加处理时间戳
    add_field => { "processed_at" => "%{+YYYY-MM-dd'T'HH:mm:ss.SSSZ}" }
  }

  # 数据验证
  if ![message] or [message] == "" {
    drop { }
  }

  # 添加索引模板字段
  mutate {
    add_field => {
      "[@metadata][index_prefix]" => "aiops-logs"
      "[@metadata][document_type]" => "log"
    }
  }

  # 根据日志类型设置不同的索引
  if "error" in [tags] {
    mutate {
      replace => { "[@metadata][index_prefix]" => "aiops-errors" }
    }
  } else if "metrics" in [tags] {
    mutate {
      replace => { "[@metadata][index_prefix]" => "aiops-metrics" }
    }
  } else if "security" in [tags] {
    mutate {
      replace => { "[@metadata][index_prefix]" => "aiops-security" }
    }
  } else if "audit" in [tags] {
    mutate {
      replace => { "[@metadata][index_prefix]" => "aiops-audit" }
    }
  }

  # 异常检测预处理
  if [metric_value] {
    # 为AI异常检测准备数据
    mutate {
      add_field => {
        "[@metadata][ai_processing]" => "true"
        "ai_metric_name" => "%{metric_name}"
        "ai_metric_value" => "%{metric_value}"
        "ai_timestamp" => "%{@timestamp}"
      }
    }
  }
}

# ===========================================
# 输出配置 (Output)
# ===========================================
output {
  # 主要输出到Elasticsearch
  elasticsearch {
    hosts => ["${ELASTICSEARCH_HOSTS:elasticsearch:9200}"]
    index => "%{[@metadata][index_prefix]}-%{+YYYY.MM.dd}"
    template_name => "aiops-template"
    template_pattern => "aiops-*"
    template => "/usr/share/logstash/templates/aiops-template.json"
    template_overwrite => true
    
    # 认证配置
    # user => "${ELASTICSEARCH_USERNAME:elastic}"
    # password => "${ELASTICSEARCH_PASSWORD:changeme}"
    
    # SSL配置
    # ssl => true
    # ssl_certificate_verification => true
    # cacert => "/usr/share/logstash/config/certs/ca.crt"
    
    # 性能配置
    workers => 4
    flush_size => 500
    idle_flush_time => 1
    
    # 错误处理
    manage_template => true
    retry_on_conflict => 3
    
    # 文档ID配置
    document_id => "%{[@metadata][fingerprint]}"
    
    # 管道配置
    pipeline => "aiops-ingest-pipeline"
  }

  # 错误日志单独输出
  if "error" in [tags] {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOSTS:elasticsearch:9200}"]
      index => "aiops-errors-%{+YYYY.MM.dd}"
      workers => 2
    }
  }

  # 安全事件输出到专门的索引
  if "security_event" in [tags] {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOSTS:elasticsearch:9200}"]
      index => "aiops-security-%{+YYYY.MM.dd}"
      workers => 2
    }
  }

  # 指标数据输出
  if "metrics" in [tags] {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOSTS:elasticsearch:9200}"]
      index => "aiops-metrics-%{+YYYY.MM.dd}"
      workers => 2
    }
  }

  # 发送告警到Redis队列
  if "alert" in [tags] {
    redis {
      host => "redis"
      port => 6379
      key => "alerts"
      data_type => "list"
      codec => "json"
    }
  }

  # 发送AI处理数据到Kafka
  if [@metadata][ai_processing] == "true" {
    kafka {
      bootstrap_servers => "kafka:9092"
      topic_id => "ai-processing"
      codec => "json"
    }
  }

  # 发送到外部SIEM系统
  if "security_event" in [tags] {
    http {
      url => "${SIEM_WEBHOOK_URL}"
      http_method => "post"
      format => "json"
      headers => {
        "Authorization" => "Bearer ${SIEM_API_TOKEN}"
        "Content-Type" => "application/json"
      }
    }
  }

  # 调试输出（开发环境）
  if [@metadata][debug] == "true" {
    stdout {
      codec => rubydebug {
        metadata => true
      }
    }
  }

  # 文件输出（备份）
  if "backup" in [tags] {
    file {
      path => "/usr/share/logstash/backup/logs-%{+YYYY-MM-dd}.log"
      codec => "json_lines"
    }
  }

  # 死信队列输出
  if "_logstashparsefailure" in [tags] {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOSTS:elasticsearch:9200}"]
      index => "aiops-failed-%{+YYYY.MM.dd}"
      workers => 1
    }
  }

  # 统计输出到InfluxDB
  if "statistics" in [tags] {
    influxdb {
      host => "influxdb"
      port => 8086
      database => "aiops_stats"
      measurement => "log_statistics"
      send_as_tags => ["host", "service", "log_level"]
    }
  }

  # Webhook通知
  if "critical" in [tags] {
    http {
      url => "${WEBHOOK_URL}"
      http_method => "post"
      format => "json"
      mapping => {
        "text" => "Critical alert: %{message}"
        "timestamp" => "%{@timestamp}"
        "host" => "%{host}"
        "service" => "%{service}"
      }
    }
  }
}
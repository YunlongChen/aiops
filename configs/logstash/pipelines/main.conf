# Logstash主管道配置
# AIOps-Platform 日志处理管道
# 版本: 1.0.0

#================================ Input ======================================

input {
  # Beats输入 - 接收来自Filebeat和Metricbeat的数据
  beats {
    port => 5044
    host => "0.0.0.0"
    client_inactivity_timeout => 300
    include_codec_tag => false
  }
  
  # HTTP输入 - 接收来自应用程序的直接日志
  http {
    port => 8080
    host => "0.0.0.0"
    codec => "json"
    additional_codecs => {
      "application/json" => "json"
      "text/plain" => "plain"
    }
    threads => 4
    max_pending_requests => 200
    max_content_length => 104857600
    response_headers => {
      "Access-Control-Allow-Origin" => "*"
      "Access-Control-Allow-Methods" => "GET, POST, DELETE, PUT"
      "Access-Control-Allow-Headers" => "authorization, content-type"
      "content-type" => "application/json"
    }
  }
  
  # TCP输入 - 接收来自应用程序的TCP日志
  tcp {
    port => 5000
    host => "0.0.0.0"
    codec => "json_lines"
    tcp_keep_alive => true
  }
  
  # UDP输入 - 接收来自系统日志的UDP数据
  udp {
    port => 5514
    host => "0.0.0.0"
    codec => "json"
    workers => 2
    queue_size => 2000
    receive_buffer_bytes => 1048576
  }
  
  # Redis输入 - 从Redis队列读取日志
  redis {
    host => "redis"
    port => 6379
    key => "logstash"
    data_type => "list"
    codec => "json"
    threads => 1
    batch_count => 125
    timeout => 5
  }
  
  # Kafka输入 - 从Kafka主题读取日志
  kafka {
    bootstrap_servers => "kafka:9092"
    topics => ["aiops-logs", "aiops-metrics", "aiops-traces"]
    group_id => "logstash-aiops"
    consumer_threads => 3
    decorate_events => true
    codec => "json"
    auto_offset_reset => "latest"
    enable_auto_commit => true
    auto_commit_interval_ms => 1000
    session_timeout_ms => 30000
    heartbeat_interval_ms => 3000
    max_poll_records => 500
    max_poll_interval_ms => 300000
  }
  
  # JDBC输入 - 从数据库读取数据
  jdbc {
    jdbc_driver_library => "/usr/share/logstash/logstash-core/lib/jars/postgresql.jar"
    jdbc_driver_class => "org.postgresql.Driver"
    jdbc_connection_string => "jdbc:postgresql://postgres:5432/aiops"
    jdbc_user => "aiops"
    jdbc_password => "aiops_password"
    schedule => "*/5 * * * *"
    statement => "SELECT * FROM logs WHERE created_at > :sql_last_value ORDER BY created_at ASC"
    use_column_value => true
    tracking_column => "created_at"
    tracking_column_type => "timestamp"
    clean_run => false
    record_last_run => true
  }
}

#================================ Filter =====================================

filter {
  # 添加处理时间戳
  mutate {
    add_field => { "[@metadata][processed_at]" => "%{+YYYY-MM-dd HH:mm:ss.SSS}" }
    add_field => { "[@metadata][pipeline]" => "main" }
  }
  
  # 根据输入类型进行不同处理
  if [fields][logtype] {
    mutate {
      add_field => { "[@metadata][logtype]" => "%{[fields][logtype]}" }
    }
  }
  
  # 处理Beats输入的数据
  if [beat] {
    # 设置索引模式
    if [beat][name] == "filebeat" {
      mutate {
        add_field => { "[@metadata][index]" => "filebeat-%{+YYYY.MM.dd}" }
        add_field => { "[@metadata][type]" => "filebeat" }
      }
    } else if [beat][name] == "metricbeat" {
      mutate {
        add_field => { "[@metadata][index]" => "metricbeat-%{+YYYY.MM.dd}" }
        add_field => { "[@metadata][type]" => "metricbeat" }
      }
    } else if [beat][name] == "heartbeat" {
      mutate {
        add_field => { "[@metadata][index]" => "heartbeat-%{+YYYY.MM.dd}" }
        add_field => { "[@metadata][type]" => "heartbeat" }
      }
    }
  }
  
  # 处理应用程序日志
  if [@metadata][logtype] == "application" or [fields][service] {
    # 解析JSON格式的日志
    if [message] =~ /^\{.*\}$/ {
      json {
        source => "message"
        target => "app"
        skip_on_invalid_json => true
      }
    }
    
    # 提取日志级别
    if [app][level] {
      mutate {
        add_field => { "log_level" => "%{[app][level]}" }
      }
    } else {
      grok {
        match => { "message" => "%{WORD:log_level}" }
        tag_on_failure => ["_grokparsefailure_loglevel"]
      }
    }
    
    # 标准化日志级别
    translate {
      field => "log_level"
      destination => "log_level_normalized"
      dictionary => {
        "TRACE" => "trace"
        "DEBUG" => "debug"
        "INFO" => "info"
        "WARN" => "warning"
        "WARNING" => "warning"
        "ERROR" => "error"
        "FATAL" => "fatal"
        "CRITICAL" => "fatal"
      }
      fallback => "unknown"
    }
    
    # 设置索引
    mutate {
      add_field => { "[@metadata][index]" => "aiops-app-%{+YYYY.MM.dd}" }
      add_field => { "[@metadata][type]" => "application" }
    }
  }
  
  # 处理系统日志
  if [@metadata][logtype] == "system" or [fields][logtype] == "system" {
    # 解析syslog格式
    grok {
      match => { 
        "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{IPORHOST:syslog_server} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}"
      }
      tag_on_failure => ["_grokparsefailure_syslog"]
    }
    
    # 解析时间戳
    if [syslog_timestamp] {
      date {
        match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
        target => "@timestamp"
      }
    }
    
    # 设置索引
    mutate {
      add_field => { "[@metadata][index]" => "aiops-system-%{+YYYY.MM.dd}" }
      add_field => { "[@metadata][type]" => "system" }
    }
  }
  
  # 处理Docker容器日志
  if [container] {
    # 提取容器信息
    mutate {
      add_field => { "container_name" => "%{[container][name]}" }
      add_field => { "container_id" => "%{[container][id]}" }
      add_field => { "container_image" => "%{[container][image][name]}" }
    }
    
    # 解析Docker日志时间戳
    if [container][labels]["com.docker.compose.service"] {
      mutate {
        add_field => { "docker_service" => "%{[container][labels][com.docker.compose.service]}" }
      }
    }
    
    # 设置索引
    mutate {
      add_field => { "[@metadata][index]" => "aiops-docker-%{+YYYY.MM.dd}" }
      add_field => { "[@metadata][type]" => "docker" }
    }
  }
  
  # 处理Kubernetes日志
  if [kubernetes] {
    # 提取Kubernetes信息
    mutate {
      add_field => { "k8s_namespace" => "%{[kubernetes][namespace]}" }
      add_field => { "k8s_pod" => "%{[kubernetes][pod][name]}" }
      add_field => { "k8s_container" => "%{[kubernetes][container][name]}" }
    }
    
    # 设置索引
    mutate {
      add_field => { "[@metadata][index]" => "aiops-k8s-%{+YYYY.MM.dd}" }
      add_field => { "[@metadata][type]" => "kubernetes" }
    }
  }
  
  # 处理Web访问日志
  if [@metadata][logtype] == "access" or [fields][logtype] == "access" {
    # 解析Apache/Nginx访问日志
    grok {
      match => { 
        "message" => "%{COMBINEDAPACHELOG}"
      }
      tag_on_failure => ["_grokparsefailure_access"]
    }
    
    # 解析用户代理
    if [agent] {
      useragent {
        source => "agent"
        target => "user_agent"
      }
    }
    
    # 解析地理位置
    if [clientip] {
      geoip {
        source => "clientip"
        target => "geoip"
        database => "/usr/share/logstash/vendor/geoip/GeoLite2-City.mmdb"
      }
    }
    
    # 转换响应大小
    if [bytes] {
      mutate {
        convert => { "bytes" => "integer" }
      }
    }
    
    # 转换响应码
    if [response] {
      mutate {
        convert => { "response" => "integer" }
      }
    }
    
    # 设置索引
    mutate {
      add_field => { "[@metadata][index]" => "aiops-access-%{+YYYY.MM.dd}" }
      add_field => { "[@metadata][type]" => "access" }
    }
  }
  
  # 处理错误日志
  if [@metadata][logtype] == "error" or [fields][logtype] == "error" or [log_level_normalized] in ["error", "fatal"] {
    # 提取堆栈跟踪
    if [message] =~ /Exception|Error|Stack/ {
      mutate {
        add_tag => ["exception"]
      }
      
      # 多行处理堆栈跟踪
      multiline {
        pattern => "^\s"
        what => "previous"
        negate => false
      }
    }
    
    # 设置索引
    mutate {
      add_field => { "[@metadata][index]" => "aiops-error-%{+YYYY.MM.dd}" }
      add_field => { "[@metadata][type]" => "error" }
    }
  }
  
  # 处理性能指标
  if [@metadata][logtype] == "metrics" or [fields][logtype] == "metrics" {
    # 解析指标数据
    if [message] =~ /^\{.*\}$/ {
      json {
        source => "message"
        target => "metrics"
        skip_on_invalid_json => true
      }
    }
    
    # 转换数值字段
    if [metrics] {
      ruby {
        code => "
          metrics = event.get('[metrics]')
          if metrics.is_a?(Hash)
            metrics.each do |key, value|
              if value.is_a?(String) && value.match(/^\d+(\.\d+)?$/)
                event.set('[metrics][' + key + ']', value.to_f)
              end
            end
          end
        "
      }
    }
    
    # 设置索引
    mutate {
      add_field => { "[@metadata][index]" => "aiops-metrics-%{+YYYY.MM.dd}" }
      add_field => { "[@metadata][type]" => "metrics" }
    }
  }
  
  # 处理安全日志
  if [@metadata][logtype] == "security" or [fields][logtype] == "security" {
    # 检测安全事件
    if [message] =~ /(?i)(failed|unauthorized|forbidden|attack|intrusion|malware|virus)/ {
      mutate {
        add_tag => ["security_event"]
      }
    }
    
    # 提取IP地址
    grok {
      match => { "message" => "%{IP:source_ip}" }
      tag_on_failure => ["_grokparsefailure_ip"]
    }
    
    # 地理位置解析
    if [source_ip] {
      geoip {
        source => "source_ip"
        target => "geoip"
        database => "/usr/share/logstash/vendor/geoip/GeoLite2-City.mmdb"
      }
    }
    
    # 设置索引
    mutate {
      add_field => { "[@metadata][index]" => "aiops-security-%{+YYYY.MM.dd}" }
      add_field => { "[@metadata][type]" => "security" }
    }
  }
  
  # 通用字段处理
  # 添加主机信息
  if ![host] {
    mutate {
      add_field => { "host" => "%{[@metadata][beat]}" }
    }
  }
  
  # 标准化时间戳
  if [@timestamp] {
    date {
      match => [ "@timestamp", "ISO8601" ]
      target => "@timestamp"
    }
  }
  
  # 添加处理标签
  mutate {
    add_tag => ["processed_by_logstash"]
    add_field => { "pipeline" => "main" }
  }
  
  # 清理临时字段
  mutate {
    remove_field => [ "beat", "input_type", "offset", "prospector" ]
  }
  
  # 字段重命名和标准化
  if [host][name] {
    mutate {
      add_field => { "hostname" => "%{[host][name]}" }
    }
  }
  
  # 添加环境标签
  mutate {
    add_field => { "environment" => "production" }
    add_field => { "platform" => "aiops" }
  }
  
  # 数据质量检查
  if ![message] and ![log] {
    mutate {
      add_tag => ["_dataquality_missing_message"]
    }
  }
  
  # 敏感数据脱敏
  mutate {
    gsub => [
      "message", "password=[^\s]+", "password=***",
      "message", "token=[^\s]+", "token=***",
      "message", "key=[^\s]+", "key=***",
      "message", "secret=[^\s]+", "secret=***"
    ]
  }
}

#================================ Output =====================================

output {
  # 输出到Elasticsearch
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    
    # 动态索引名称
    index => "%{[@metadata][index]:aiops-logs-%{+YYYY.MM.dd}}"
    
    # 文档类型
    document_type => "%{[@metadata][type]:_doc}"
    
    # 模板设置
    template_name => "aiops-logs"
    template_pattern => "aiops-*"
    template_overwrite => true
    template => "/usr/share/logstash/templates/aiops-template.json"
    
    # 批处理设置
    flush_size => 1000
    idle_flush_time => 1
    
    # 重试设置
    retry_on_conflict => 3
    retry_on_failure => 3
    
    # 健康检查
    healthcheck_path => "/"
    
    # 认证
    # user => "logstash_writer"
    # password => "logstash_password"
    
    # SSL设置
    # ssl => true
    # ssl_certificate_verification => true
    # cacert => "/path/to/ca.crt"
    
    # 管道
    pipeline => "aiops-pipeline"
  }
  
  # 输出到Redis（用于缓存和队列）
  redis {
    host => "redis"
    port => 6379
    key => "processed_logs"
    data_type => "list"
    codec => "json"
  }
  
  # 输出到Kafka（用于流处理）
  kafka {
    bootstrap_servers => "kafka:9092"
    topic_id => "processed-logs"
    codec => "json"
    compression_type => "snappy"
    acks => "1"
    retries => 3
    batch_size => 16384
    linger_ms => 5
    buffer_memory => 33554432
  }
  
  # 条件输出 - 错误日志到专门的索引
  if "error" in [tags] or [log_level_normalized] in ["error", "fatal"] {
    elasticsearch {
      hosts => ["http://elasticsearch:9200"]
      index => "aiops-errors-%{+YYYY.MM.dd}"
      document_type => "error"
    }
  }
  
  # 条件输出 - 安全事件到专门的索引
  if "security_event" in [tags] {
    elasticsearch {
      hosts => ["http://elasticsearch:9200"]
      index => "aiops-security-%{+YYYY.MM.dd}"
      document_type => "security"
    }
    
    # 同时发送到安全监控系统
    http {
      url => "http://security-monitor:8080/events"
      http_method => "post"
      format => "json"
      headers => {
        "Content-Type" => "application/json"
        "Authorization" => "Bearer ${SECURITY_TOKEN}"
      }
    }
  }
  
  # 调试输出（仅在开发环境）
  if [environment] == "development" {
    stdout {
      codec => rubydebug {
        metadata => true
      }
    }
  }
  
  # 文件输出（用于备份）
  file {
    path => "/var/log/logstash/processed/%{[@metadata][type]:unknown}/%{+YYYY}/%{+MM}/%{+dd}/logstash-%{+HH}.log"
    codec => "json_lines"
    file_mode => 0644
    dir_mode => 0755
    flush_interval => 60
  }
}
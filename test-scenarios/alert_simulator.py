#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‘Šè­¦ç³»ç»Ÿæ¨¡æ‹Ÿå™¨

è¯¥æ¨¡å—æ¨¡æ‹Ÿå‘Šè­¦ç³»ç»Ÿçš„å„ç§åŠŸèƒ½ï¼ŒåŒ…æ‹¬å‘Šè­¦è§„åˆ™ã€é˜ˆå€¼æ£€æµ‹ã€å‘Šè­¦é€šçŸ¥ç­‰ã€‚
æ”¯æŒå¤šç§å‘Šè­¦ç±»å‹å’Œé€šçŸ¥æ¸ é“çš„æ¨¡æ‹Ÿã€‚

Author: AIOps Team
Date: 2025-01-10
"""

import json
import time
import random
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, field
from enum import Enum
import logging
import uuid


class AlertSeverity(Enum):
    """å‘Šè­¦ä¸¥é‡çº§åˆ«æšä¸¾"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    INFO = "info"


class AlertStatus(Enum):
    """å‘Šè­¦çŠ¶æ€æšä¸¾"""
    FIRING = "firing"
    RESOLVED = "resolved"
    SUPPRESSED = "suppressed"
    ACKNOWLEDGED = "acknowledged"


class NotificationChannel(Enum):
    """é€šçŸ¥æ¸ é“æšä¸¾"""
    EMAIL = "email"
    SMS = "sms"
    SLACK = "slack"
    WEBHOOK = "webhook"
    PAGERDUTY = "pagerduty"
    DINGTALK = "dingtalk"


class MetricType(Enum):
    """æŒ‡æ ‡ç±»å‹æšä¸¾"""
    CPU_USAGE = "cpu_usage"
    MEMORY_USAGE = "memory_usage"
    DISK_USAGE = "disk_usage"
    NETWORK_USAGE = "network_usage"
    RESPONSE_TIME = "response_time"
    ERROR_RATE = "error_rate"
    THROUGHPUT = "throughput"
    AVAILABILITY = "availability"


@dataclass
class AlertRule:
    """å‘Šè­¦è§„åˆ™æ•°æ®ç±»"""
    id: str
    name: str
    description: str
    metric_type: MetricType
    condition: str  # >, <, >=, <=, ==, !=
    threshold: float
    duration_seconds: int  # æŒç»­æ—¶é—´
    severity: AlertSeverity
    enabled: bool = True
    tags: Dict[str, str] = field(default_factory=dict)
    notification_channels: List[NotificationChannel] = field(default_factory=list)
    

@dataclass
class Alert:
    """å‘Šè­¦æ•°æ®ç±»"""
    id: str
    rule_id: str
    rule_name: str
    severity: AlertSeverity
    status: AlertStatus
    message: str
    description: str
    source: str  # å‘Šè­¦æ¥æºï¼ˆä¸»æœºåã€æœåŠ¡åç­‰ï¼‰
    metric_name: str
    metric_value: float
    threshold: float
    condition: str
    fired_at: datetime
    resolved_at: Optional[datetime] = None
    acknowledged_at: Optional[datetime] = None
    acknowledged_by: Optional[str] = None
    tags: Dict[str, str] = field(default_factory=dict)
    annotations: Dict[str, str] = field(default_factory=dict)


@dataclass
class NotificationRecord:
    """é€šçŸ¥è®°å½•æ•°æ®ç±»"""
    id: str
    alert_id: str
    channel: NotificationChannel
    recipient: str
    subject: str
    content: str
    sent_at: datetime
    success: bool
    error_message: Optional[str] = None
    retry_count: int = 0


class AlertSimulator:
    """å‘Šè­¦ç³»ç»Ÿæ¨¡æ‹Ÿå™¨"""
    
    def __init__(self, config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–å‘Šè­¦æ¨¡æ‹Ÿå™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config or self._get_default_config()
        self.alert_rules = self._load_alert_rules()
        self.active_alerts = {}  # alert_id -> Alert
        self.alert_history = []  # å†å²å‘Šè­¦è®°å½•
        self.notification_records = []  # é€šçŸ¥è®°å½•
        self.running = False
        self.simulation_threads = []
        self.logger = self._setup_logger()
        
        # æ¨¡æ‹Ÿæ•°æ®æº
        self.metric_sources = self._generate_metric_sources()
        
        # é€šçŸ¥å›è°ƒå‡½æ•°
        self.notification_callbacks = {}
        
    def _get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "evaluation": {
                "interval_seconds": 15,
                "max_alerts_per_rule": 10,
                "alert_timeout_hours": 24
            },
            "notification": {
                "retry_attempts": 3,
                "retry_delay_seconds": 60,
                "rate_limit_per_minute": 10,
                "group_wait_seconds": 30,
                "group_interval_seconds": 300,
                "repeat_interval_hours": 4
            },
            "channels": {
                "email": {
                    "enabled": True,
                    "smtp_server": "smtp.example.com",
                    "from_address": "alerts@example.com",
                    "recipients": ["admin@example.com", "ops@example.com"]
                },
                "slack": {
                    "enabled": True,
                    "webhook_url": "https://hooks.slack.com/services/...",
                    "channel": "#alerts",
                    "username": "AlertBot"
                },
                "webhook": {
                    "enabled": True,
                    "url": "http://localhost:8080/webhook/alerts",
                    "timeout_seconds": 10
                }
            },
            "simulation": {
                "enable_random_alerts": True,
                "random_alert_probability": 0.1,
                "enable_alert_storms": True,
                "storm_probability": 0.02,
                "auto_resolve_probability": 0.3,
                "auto_resolve_delay_minutes": [5, 30]
            }
        }
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger("AlertSimulator")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def _load_alert_rules(self) -> List[AlertRule]:
        """åŠ è½½å‘Šè­¦è§„åˆ™"""
        rules = [
            # CPUç›¸å…³å‘Šè­¦
            AlertRule(
                id="cpu_high",
                name="High CPU Usage",
                description="CPU usage is above threshold",
                metric_type=MetricType.CPU_USAGE,
                condition=">",
                threshold=80.0,
                duration_seconds=300,
                severity=AlertSeverity.HIGH,
                tags={"team": "infrastructure", "service": "system"},
                notification_channels=[NotificationChannel.EMAIL, NotificationChannel.SLACK]
            ),
            AlertRule(
                id="cpu_critical",
                name="Critical CPU Usage",
                description="CPU usage is critically high",
                metric_type=MetricType.CPU_USAGE,
                condition=">",
                threshold=95.0,
                duration_seconds=60,
                severity=AlertSeverity.CRITICAL,
                tags={"team": "infrastructure", "service": "system"},
                notification_channels=[NotificationChannel.EMAIL, NotificationChannel.SMS, NotificationChannel.PAGERDUTY]
            ),
            
            # å†…å­˜ç›¸å…³å‘Šè­¦
            AlertRule(
                id="memory_high",
                name="High Memory Usage",
                description="Memory usage is above threshold",
                metric_type=MetricType.MEMORY_USAGE,
                condition=">",
                threshold=85.0,
                duration_seconds=300,
                severity=AlertSeverity.MEDIUM,
                tags={"team": "infrastructure", "service": "system"},
                notification_channels=[NotificationChannel.EMAIL]
            ),
            AlertRule(
                id="memory_critical",
                name="Critical Memory Usage",
                description="Memory usage is critically high",
                metric_type=MetricType.MEMORY_USAGE,
                condition=">",
                threshold=95.0,
                duration_seconds=120,
                severity=AlertSeverity.CRITICAL,
                tags={"team": "infrastructure", "service": "system"},
                notification_channels=[NotificationChannel.EMAIL, NotificationChannel.SLACK, NotificationChannel.PAGERDUTY]
            ),
            
            # ç£ç›˜ç›¸å…³å‘Šè­¦
            AlertRule(
                id="disk_high",
                name="High Disk Usage",
                description="Disk usage is above threshold",
                metric_type=MetricType.DISK_USAGE,
                condition=">",
                threshold=90.0,
                duration_seconds=600,
                severity=AlertSeverity.HIGH,
                tags={"team": "infrastructure", "service": "storage"},
                notification_channels=[NotificationChannel.EMAIL, NotificationChannel.SLACK]
            ),
            
            # åº”ç”¨ç›¸å…³å‘Šè­¦
            AlertRule(
                id="response_time_high",
                name="High Response Time",
                description="Application response time is too high",
                metric_type=MetricType.RESPONSE_TIME,
                condition=">",
                threshold=2000.0,  # 2ç§’
                duration_seconds=180,
                severity=AlertSeverity.MEDIUM,
                tags={"team": "application", "service": "web"},
                notification_channels=[NotificationChannel.EMAIL, NotificationChannel.SLACK]
            ),
            AlertRule(
                id="error_rate_high",
                name="High Error Rate",
                description="Application error rate is above threshold",
                metric_type=MetricType.ERROR_RATE,
                condition=">",
                threshold=5.0,  # 5%
                duration_seconds=120,
                severity=AlertSeverity.HIGH,
                tags={"team": "application", "service": "web"},
                notification_channels=[NotificationChannel.EMAIL, NotificationChannel.SLACK, NotificationChannel.PAGERDUTY]
            ),
            
            # å¯ç”¨æ€§å‘Šè­¦
            AlertRule(
                id="service_down",
                name="Service Unavailable",
                description="Service is not responding",
                metric_type=MetricType.AVAILABILITY,
                condition="<",
                threshold=1.0,  # æœåŠ¡ä¸å¯ç”¨
                duration_seconds=60,
                severity=AlertSeverity.CRITICAL,
                tags={"team": "sre", "service": "all"},
                notification_channels=[NotificationChannel.EMAIL, NotificationChannel.SMS, NotificationChannel.PAGERDUTY]
            )
        ]
        
        return rules
    
    def _generate_metric_sources(self) -> List[Dict]:
        """ç”ŸæˆæŒ‡æ ‡æ•°æ®æº"""
        return [
            {"name": "web-01", "type": "server", "service": "nginx"},
            {"name": "web-02", "type": "server", "service": "nginx"},
            {"name": "db-01", "type": "server", "service": "mysql"},
            {"name": "app-01", "type": "server", "service": "application"},
            {"name": "cache-01", "type": "server", "service": "redis"},
            {"name": "lb-01", "type": "server", "service": "haproxy"},
            {"name": "api-service", "type": "application", "service": "api"},
            {"name": "web-service", "type": "application", "service": "web"},
            {"name": "payment-service", "type": "application", "service": "payment"}
        ]
    
    def start_simulation(self):
        """å¯åŠ¨å‘Šè­¦æ¨¡æ‹Ÿ"""
        self.running = True
        
        # å¯åŠ¨å‘Šè­¦è¯„ä¼°çº¿ç¨‹
        evaluation_thread = threading.Thread(
            target=self._alert_evaluation_loop,
            name="AlertEvaluation"
        )
        evaluation_thread.daemon = True
        evaluation_thread.start()
        self.simulation_threads.append(evaluation_thread)
        
        # å¯åŠ¨é€šçŸ¥å¤„ç†çº¿ç¨‹
        notification_thread = threading.Thread(
            target=self._notification_loop,
            name="NotificationHandler"
        )
        notification_thread.daemon = True
        notification_thread.start()
        self.simulation_threads.append(notification_thread)
        
        # å¯åŠ¨è‡ªåŠ¨è§£å†³çº¿ç¨‹
        auto_resolve_thread = threading.Thread(
            target=self._auto_resolve_loop,
            name="AutoResolve"
        )
        auto_resolve_thread.daemon = True
        auto_resolve_thread.start()
        self.simulation_threads.append(auto_resolve_thread)
        
        self.logger.info("Alert simulation started")
    
    def stop_simulation(self):
        """åœæ­¢å‘Šè­¦æ¨¡æ‹Ÿ"""
        self.running = False
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹ç»“æŸ
        for thread in self.simulation_threads:
            thread.join(timeout=5)
        
        self.simulation_threads.clear()
        self.logger.info("Alert simulation stopped")
    
    def _alert_evaluation_loop(self):
        """å‘Šè­¦è¯„ä¼°å¾ªç¯"""
        interval = self.config["evaluation"]["interval_seconds"]
        
        while self.running:
            try:
                # è¯„ä¼°æ‰€æœ‰å¯ç”¨çš„å‘Šè­¦è§„åˆ™
                for rule in self.alert_rules:
                    if rule.enabled:
                        self._evaluate_rule(rule)
                
                # æ¨¡æ‹Ÿéšæœºå‘Šè­¦
                if self.config["simulation"]["enable_random_alerts"]:
                    self._generate_random_alerts()
                
                # æ¨¡æ‹Ÿå‘Šè­¦é£æš´
                if self.config["simulation"]["enable_alert_storms"]:
                    self._generate_alert_storm()
                
                time.sleep(interval)
                
            except Exception as e:
                self.logger.error(f"Error in alert evaluation: {e}")
                time.sleep(interval)
    
    def _notification_loop(self):
        """é€šçŸ¥å¤„ç†å¾ªç¯"""
        while self.running:
            try:
                # å¤„ç†å¾…å‘é€çš„é€šçŸ¥
                self._process_pending_notifications()
                
                time.sleep(5)  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                self.logger.error(f"Error in notification processing: {e}")
                time.sleep(5)
    
    def _auto_resolve_loop(self):
        """è‡ªåŠ¨è§£å†³å‘Šè­¦å¾ªç¯"""
        while self.running:
            try:
                # è‡ªåŠ¨è§£å†³ä¸€äº›å‘Šè­¦
                if self.config["simulation"]["auto_resolve_probability"] > 0:
                    self._auto_resolve_alerts()
                
                time.sleep(30)  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                self.logger.error(f"Error in auto resolve: {e}")
                time.sleep(30)
    
    def _evaluate_rule(self, rule: AlertRule):
        """è¯„ä¼°å‘Šè­¦è§„åˆ™"""
        # æ¨¡æ‹Ÿè·å–æŒ‡æ ‡å€¼
        for source in self.metric_sources:
            metric_value = self._simulate_metric_value(rule.metric_type, source)
            
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³å‘Šè­¦æ¡ä»¶
            if self._check_condition(metric_value, rule.condition, rule.threshold):
                # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰ç›¸åŒçš„å‘Šè­¦
                alert_key = f"{rule.id}_{source['name']}"
                
                if alert_key not in self.active_alerts:
                    # åˆ›å»ºæ–°å‘Šè­¦
                    alert = self._create_alert(rule, source, metric_value)
                    self.active_alerts[alert_key] = alert
                    self.alert_history.append(alert)
                    
                    self.logger.info(f"Alert fired: {alert.rule_name} on {alert.source}")
                    
                    # å‘é€é€šçŸ¥
                    self._schedule_notifications(alert)
                else:
                    # æ›´æ–°ç°æœ‰å‘Šè­¦çš„æŒ‡æ ‡å€¼
                    existing_alert = self.active_alerts[alert_key]
                    existing_alert.metric_value = metric_value
            else:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦è§£å†³å‘Šè­¦
                alert_key = f"{rule.id}_{source['name']}"
                if alert_key in self.active_alerts:
                    alert = self.active_alerts[alert_key]
                    self._resolve_alert(alert)
    
    def _simulate_metric_value(self, metric_type: MetricType, source: Dict) -> float:
        """æ¨¡æ‹ŸæŒ‡æ ‡å€¼"""
        # æ ¹æ®æŒ‡æ ‡ç±»å‹å’Œæ•°æ®æºç”Ÿæˆæ¨¡æ‹Ÿå€¼
        base_values = {
            MetricType.CPU_USAGE: 45.0,
            MetricType.MEMORY_USAGE: 60.0,
            MetricType.DISK_USAGE: 70.0,
            MetricType.NETWORK_USAGE: 30.0,
            MetricType.RESPONSE_TIME: 500.0,
            MetricType.ERROR_RATE: 1.0,
            MetricType.THROUGHPUT: 100.0,
            MetricType.AVAILABILITY: 1.0
        }
        
        base_value = base_values.get(metric_type, 50.0)
        
        # æ ¹æ®æœåŠ¡ç±»å‹è°ƒæ•´åŸºç¡€å€¼
        if source["service"] == "mysql" and metric_type == MetricType.CPU_USAGE:
            base_value *= 1.5  # æ•°æ®åº“CPUä½¿ç”¨ç‡é€šå¸¸æ›´é«˜
        elif source["service"] == "redis" and metric_type == MetricType.MEMORY_USAGE:
            base_value *= 1.3  # Rediså†…å­˜ä½¿ç”¨ç‡æ›´é«˜
        
        # æ·»åŠ éšæœºå˜åŒ–
        variation = random.uniform(0.7, 1.4)
        value = base_value * variation
        
        # æ¨¡æ‹Ÿå¼‚å¸¸æƒ…å†µ
        if random.random() < 0.1:  # 10%æ¦‚ç‡å‡ºç°å¼‚å¸¸å€¼
            if metric_type in [MetricType.CPU_USAGE, MetricType.MEMORY_USAGE, MetricType.DISK_USAGE]:
                value = random.uniform(85, 98)  # é«˜ä½¿ç”¨ç‡
            elif metric_type == MetricType.RESPONSE_TIME:
                value = random.uniform(3000, 10000)  # é«˜å“åº”æ—¶é—´
            elif metric_type == MetricType.ERROR_RATE:
                value = random.uniform(8, 25)  # é«˜é”™è¯¯ç‡
            elif metric_type == MetricType.AVAILABILITY:
                value = 0.0  # æœåŠ¡ä¸å¯ç”¨
        
        return round(value, 2)
    
    def _check_condition(self, value: float, condition: str, threshold: float) -> bool:
        """æ£€æŸ¥å‘Šè­¦æ¡ä»¶"""
        if condition == ">":
            return value > threshold
        elif condition == ">=":
            return value >= threshold
        elif condition == "<":
            return value < threshold
        elif condition == "<=":
            return value <= threshold
        elif condition == "==":
            return abs(value - threshold) < 0.001
        elif condition == "!=":
            return abs(value - threshold) >= 0.001
        else:
            return False
    
    def _create_alert(self, rule: AlertRule, source: Dict, metric_value: float) -> Alert:
        """åˆ›å»ºå‘Šè­¦"""
        alert_id = str(uuid.uuid4())
        
        # ç”Ÿæˆå‘Šè­¦æ¶ˆæ¯
        message = f"{rule.name}: {source['name']} {rule.metric_type.value} is {metric_value} (threshold: {rule.threshold})"
        
        # ç”Ÿæˆæè¿°
        description = f"{rule.description}. Current value: {metric_value}, Threshold: {rule.threshold}"
        
        alert = Alert(
            id=alert_id,
            rule_id=rule.id,
            rule_name=rule.name,
            severity=rule.severity,
            status=AlertStatus.FIRING,
            message=message,
            description=description,
            source=source["name"],
            metric_name=rule.metric_type.value,
            metric_value=metric_value,
            threshold=rule.threshold,
            condition=rule.condition,
            fired_at=datetime.now(),
            tags=rule.tags.copy(),
            annotations={
                "service": source["service"],
                "type": source["type"],
                "runbook": f"https://runbooks.example.com/{rule.id}",
                "dashboard": f"https://grafana.example.com/d/{source['name']}"
            }
        )
        
        return alert
    
    def _resolve_alert(self, alert: Alert):
        """è§£å†³å‘Šè­¦"""
        alert.status = AlertStatus.RESOLVED
        alert.resolved_at = datetime.now()
        
        # ä»æ´»è·ƒå‘Šè­¦ä¸­ç§»é™¤
        alert_key = f"{alert.rule_id}_{alert.source}"
        if alert_key in self.active_alerts:
            del self.active_alerts[alert_key]
        
        self.logger.info(f"Alert resolved: {alert.rule_name} on {alert.source}")
        
        # å‘é€è§£å†³é€šçŸ¥
        self._schedule_resolution_notifications(alert)
    
    def _schedule_notifications(self, alert: Alert):
        """å®‰æ’é€šçŸ¥å‘é€"""
        rule = next((r for r in self.alert_rules if r.id == alert.rule_id), None)
        if not rule:
            return
        
        for channel in rule.notification_channels:
            self._send_notification(alert, channel, "alert")
    
    def _schedule_resolution_notifications(self, alert: Alert):
        """å®‰æ’è§£å†³é€šçŸ¥å‘é€"""
        rule = next((r for r in self.alert_rules if r.id == alert.rule_id), None)
        if not rule:
            return
        
        for channel in rule.notification_channels:
            self._send_notification(alert, channel, "resolution")
    
    def _send_notification(self, alert: Alert, channel: NotificationChannel, notification_type: str):
        """å‘é€é€šçŸ¥"""
        notification_id = str(uuid.uuid4())
        
        # ç”Ÿæˆé€šçŸ¥å†…å®¹
        if notification_type == "alert":
            subject = f"ğŸš¨ {alert.severity.value.upper()}: {alert.rule_name}"
            content = self._generate_alert_content(alert)
        else:  # resolution
            subject = f"âœ… RESOLVED: {alert.rule_name}"
            content = self._generate_resolution_content(alert)
        
        # è·å–æ”¶ä»¶äºº
        recipients = self._get_recipients(channel)
        
        for recipient in recipients:
            # æ¨¡æ‹Ÿå‘é€
            success = self._simulate_send(channel, recipient, subject, content)
            
            # è®°å½•é€šçŸ¥
            record = NotificationRecord(
                id=notification_id,
                alert_id=alert.id,
                channel=channel,
                recipient=recipient,
                subject=subject,
                content=content,
                sent_at=datetime.now(),
                success=success,
                error_message=None if success else "Simulated delivery failure"
            )
            
            self.notification_records.append(record)
            
            if success:
                self.logger.info(f"Notification sent via {channel.value} to {recipient}")
            else:
                self.logger.warning(f"Notification failed via {channel.value} to {recipient}")
    
    def _generate_alert_content(self, alert: Alert) -> str:
        """ç”Ÿæˆå‘Šè­¦é€šçŸ¥å†…å®¹"""
        content = f"""
ğŸš¨ Alert: {alert.rule_name}

Severity: {alert.severity.value.upper()}
Source: {alert.source}
Metric: {alert.metric_name}
Current Value: {alert.metric_value}
Threshold: {alert.condition} {alert.threshold}
Fired At: {alert.fired_at.strftime('%Y-%m-%d %H:%M:%S')}

Description: {alert.description}

Tags: {', '.join([f'{k}={v}' for k, v in alert.tags.items()])}

Runbook: {alert.annotations.get('runbook', 'N/A')}
Dashboard: {alert.annotations.get('dashboard', 'N/A')}
        """.strip()
        
        return content
    
    def _generate_resolution_content(self, alert: Alert) -> str:
        """ç”Ÿæˆè§£å†³é€šçŸ¥å†…å®¹"""
        duration = ""
        if alert.resolved_at and alert.fired_at:
            duration_seconds = (alert.resolved_at - alert.fired_at).total_seconds()
            duration = f"Duration: {int(duration_seconds // 60)}m {int(duration_seconds % 60)}s"
        
        content = f"""
âœ… Alert Resolved: {alert.rule_name}

Source: {alert.source}
Metric: {alert.metric_name}
Fired At: {alert.fired_at.strftime('%Y-%m-%d %H:%M:%S')}
Resolved At: {alert.resolved_at.strftime('%Y-%m-%d %H:%M:%S') if alert.resolved_at else 'N/A'}
{duration}

The alert condition is no longer met.
        """.strip()
        
        return content
    
    def _get_recipients(self, channel: NotificationChannel) -> List[str]:
        """è·å–é€šçŸ¥æ”¶ä»¶äºº"""
        channel_config = self.config["channels"].get(channel.value, {})
        
        if channel == NotificationChannel.EMAIL:
            return channel_config.get("recipients", ["admin@example.com"])
        elif channel == NotificationChannel.SMS:
            return ["+1234567890", "+0987654321"]
        elif channel == NotificationChannel.SLACK:
            return [channel_config.get("channel", "#alerts")]
        elif channel == NotificationChannel.WEBHOOK:
            return [channel_config.get("url", "http://localhost:8080/webhook")]
        elif channel == NotificationChannel.PAGERDUTY:
            return ["integration-key-123"]
        elif channel == NotificationChannel.DINGTALK:
            return ["dingtalk-webhook-url"]
        else:
            return ["default-recipient"]
    
    def _simulate_send(self, channel: NotificationChannel, recipient: str, subject: str, content: str) -> bool:
        """æ¨¡æ‹Ÿå‘é€é€šçŸ¥"""
        # æ¨¡æ‹Ÿå‘é€æˆåŠŸç‡
        success_rates = {
            NotificationChannel.EMAIL: 0.95,
            NotificationChannel.SMS: 0.90,
            NotificationChannel.SLACK: 0.98,
            NotificationChannel.WEBHOOK: 0.85,
            NotificationChannel.PAGERDUTY: 0.99,
            NotificationChannel.DINGTALK: 0.92
        }
        
        success_rate = success_rates.get(channel, 0.90)
        return random.random() < success_rate
    
    def _process_pending_notifications(self):
        """å¤„ç†å¾…å‘é€çš„é€šçŸ¥"""
        # è¿™é‡Œå¯ä»¥å®ç°é€šçŸ¥é‡è¯•ã€é™æµç­‰é€»è¾‘
        # å½“å‰åªæ˜¯æ¨¡æ‹Ÿï¼Œå®é™…å®ç°ä¼šæ›´å¤æ‚
        pass
    
    def _generate_random_alerts(self):
        """ç”Ÿæˆéšæœºå‘Šè­¦"""
        probability = self.config["simulation"]["random_alert_probability"]
        
        if random.random() < probability:
            # éšæœºé€‰æ‹©ä¸€ä¸ªè§„åˆ™å’Œæ•°æ®æº
            rule = random.choice(self.alert_rules)
            source = random.choice(self.metric_sources)
            
            # ç”Ÿæˆæ»¡è¶³å‘Šè­¦æ¡ä»¶çš„æŒ‡æ ‡å€¼
            if rule.condition == ">":
                metric_value = rule.threshold + random.uniform(1, 20)
            elif rule.condition == "<":
                metric_value = rule.threshold - random.uniform(0.1, 1)
            else:
                metric_value = rule.threshold
            
            # åˆ›å»ºå‘Šè­¦
            alert_key = f"{rule.id}_{source['name']}"
            if alert_key not in self.active_alerts:
                alert = self._create_alert(rule, source, metric_value)
                self.active_alerts[alert_key] = alert
                self.alert_history.append(alert)
                
                self.logger.info(f"Random alert generated: {alert.rule_name} on {alert.source}")
                self._schedule_notifications(alert)
    
    def _generate_alert_storm(self):
        """ç”Ÿæˆå‘Šè­¦é£æš´"""
        probability = self.config["simulation"]["storm_probability"]
        
        if random.random() < probability:
            self.logger.warning("Alert storm detected! Generating multiple alerts...")
            
            # ç”Ÿæˆå¤šä¸ªå‘Šè­¦
            storm_size = random.randint(5, 15)
            for _ in range(storm_size):
                rule = random.choice(self.alert_rules)
                source = random.choice(self.metric_sources)
                
                # ç”Ÿæˆå‘Šè­¦æ¡ä»¶å€¼
                if rule.condition == ">":
                    metric_value = rule.threshold + random.uniform(10, 50)
                else:
                    metric_value = rule.threshold - random.uniform(0.5, 2)
                
                alert_key = f"{rule.id}_{source['name']}_storm_{random.randint(1000, 9999)}"
                alert = self._create_alert(rule, source, metric_value)
                alert.tags["storm"] = "true"
                
                self.active_alerts[alert_key] = alert
                self.alert_history.append(alert)
                self._schedule_notifications(alert)
    
    def _auto_resolve_alerts(self):
        """è‡ªåŠ¨è§£å†³ä¸€äº›å‘Šè­¦"""
        probability = self.config["simulation"]["auto_resolve_probability"]
        
        # è·å–å¯ä»¥è‡ªåŠ¨è§£å†³çš„å‘Šè­¦
        resolvable_alerts = []
        for alert in self.active_alerts.values():
            if alert.status == AlertStatus.FIRING:
                # æ£€æŸ¥å‘Šè­¦æŒç»­æ—¶é—´
                duration_minutes = (datetime.now() - alert.fired_at).total_seconds() / 60
                min_duration, max_duration = self.config["simulation"]["auto_resolve_delay_minutes"]
                
                if min_duration <= duration_minutes <= max_duration:
                    resolvable_alerts.append(alert)
        
        # éšæœºè§£å†³ä¸€äº›å‘Šè­¦
        for alert in resolvable_alerts:
            if random.random() < probability:
                self._resolve_alert(alert)
    
    def acknowledge_alert(self, alert_id: str, acknowledged_by: str) -> bool:
        """ç¡®è®¤å‘Šè­¦"""
        # åœ¨æ´»è·ƒå‘Šè­¦ä¸­æŸ¥æ‰¾
        for alert in self.active_alerts.values():
            if alert.id == alert_id:
                alert.status = AlertStatus.ACKNOWLEDGED
                alert.acknowledged_at = datetime.now()
                alert.acknowledged_by = acknowledged_by
                
                self.logger.info(f"Alert acknowledged: {alert.rule_name} by {acknowledged_by}")
                return True
        
        return False
    
    def suppress_alert(self, alert_id: str) -> bool:
        """æŠ‘åˆ¶å‘Šè­¦"""
        for alert in self.active_alerts.values():
            if alert.id == alert_id:
                alert.status = AlertStatus.SUPPRESSED
                self.logger.info(f"Alert suppressed: {alert.rule_name}")
                return True
        
        return False
    
    def get_active_alerts(self, severity: Optional[AlertSeverity] = None) -> List[Alert]:
        """è·å–æ´»è·ƒå‘Šè­¦"""
        alerts = list(self.active_alerts.values())
        
        if severity:
            alerts = [a for a in alerts if a.severity == severity]
        
        return sorted(alerts, key=lambda x: x.fired_at, reverse=True)
    
    def get_alert_history(self, hours: int = 24) -> List[Alert]:
        """è·å–å‘Šè­¦å†å²"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        return [a for a in self.alert_history if a.fired_at >= cutoff_time]
    
    def get_notification_records(self, hours: int = 24) -> List[NotificationRecord]:
        """è·å–é€šçŸ¥è®°å½•"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        return [n for n in self.notification_records if n.sent_at >= cutoff_time]
    
    def export_alerts_to_json(self, filename: str):
        """å¯¼å‡ºå‘Šè­¦åˆ°JSONæ–‡ä»¶"""
        data = {
            "active_alerts": [],
            "alert_history": [],
            "notification_records": [],
            "exported_at": datetime.now().isoformat()
        }
        
        # è½¬æ¢æ´»è·ƒå‘Šè­¦
        for alert in self.active_alerts.values():
            data["active_alerts"].append(self._alert_to_dict(alert))
        
        # è½¬æ¢å†å²å‘Šè­¦
        for alert in self.alert_history:
            data["alert_history"].append(self._alert_to_dict(alert))
        
        # è½¬æ¢é€šçŸ¥è®°å½•
        for record in self.notification_records:
            data["notification_records"].append({
                "id": record.id,
                "alert_id": record.alert_id,
                "channel": record.channel.value,
                "recipient": record.recipient,
                "subject": record.subject,
                "content": record.content,
                "sent_at": record.sent_at.isoformat(),
                "success": record.success,
                "error_message": record.error_message,
                "retry_count": record.retry_count
            })
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"Alerts exported to {filename}")
    
    def _alert_to_dict(self, alert: Alert) -> Dict:
        """å°†å‘Šè­¦å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "id": alert.id,
            "rule_id": alert.rule_id,
            "rule_name": alert.rule_name,
            "severity": alert.severity.value,
            "status": alert.status.value,
            "message": alert.message,
            "description": alert.description,
            "source": alert.source,
            "metric_name": alert.metric_name,
            "metric_value": alert.metric_value,
            "threshold": alert.threshold,
            "condition": alert.condition,
            "fired_at": alert.fired_at.isoformat(),
            "resolved_at": alert.resolved_at.isoformat() if alert.resolved_at else None,
            "acknowledged_at": alert.acknowledged_at.isoformat() if alert.acknowledged_at else None,
            "acknowledged_by": alert.acknowledged_by,
            "tags": alert.tags,
            "annotations": alert.annotations
        }
    
    def generate_alert_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆå‘Šè­¦æŠ¥å‘Š"""
        now = datetime.now()
        last_24h = now - timedelta(hours=24)
        
        # ç»Ÿè®¡æ´»è·ƒå‘Šè­¦
        active_alerts = list(self.active_alerts.values())
        active_by_severity = {}
        for severity in AlertSeverity:
            active_by_severity[severity.value] = len([a for a in active_alerts if a.severity == severity])
        
        # ç»Ÿè®¡24å°æ—¶å†…çš„å‘Šè­¦
        recent_alerts = [a for a in self.alert_history if a.fired_at >= last_24h]
        total_alerts_24h = len(recent_alerts)
        resolved_alerts_24h = len([a for a in recent_alerts if a.status == AlertStatus.RESOLVED])
        
        # ç»Ÿè®¡é€šçŸ¥
        recent_notifications = [n for n in self.notification_records if n.sent_at >= last_24h]
        total_notifications = len(recent_notifications)
        successful_notifications = len([n for n in recent_notifications if n.success])
        
        # æŒ‰æ¸ é“ç»Ÿè®¡é€šçŸ¥
        notifications_by_channel = {}
        for channel in NotificationChannel:
            channel_notifications = [n for n in recent_notifications if n.channel == channel]
            notifications_by_channel[channel.value] = {
                "total": len(channel_notifications),
                "successful": len([n for n in channel_notifications if n.success])
            }
        
        # æœ€é¢‘ç¹çš„å‘Šè­¦è§„åˆ™
        rule_counts = {}
        for alert in recent_alerts:
            rule_counts[alert.rule_name] = rule_counts.get(alert.rule_name, 0) + 1
        
        top_rules = sorted(rule_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        
        return {
            "summary": {
                "active_alerts": len(active_alerts),
                "alerts_last_24h": total_alerts_24h,
                "resolved_last_24h": resolved_alerts_24h,
                "resolution_rate": resolved_alerts_24h / total_alerts_24h if total_alerts_24h > 0 else 0,
                "notifications_sent": total_notifications,
                "notification_success_rate": successful_notifications / total_notifications if total_notifications > 0 else 0
            },
            "active_alerts_by_severity": active_by_severity,
            "notifications_by_channel": notifications_by_channel,
            "top_alert_rules": dict(top_rules),
            "generated_at": now.isoformat()
        }


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Alert System Simulator")
    parser.add_argument("--config", help="Configuration file path")
    parser.add_argument("--duration", type=int, default=300, help="Simulation duration in seconds")
    parser.add_argument("--export", help="Export alerts to JSON file")
    parser.add_argument("--report", help="Generate alert report file")
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = None
    if args.config:
        try:
            with open(args.config, 'r', encoding='utf-8') as f:
                config = json.load(f)
        except Exception as e:
            print(f"Failed to load config: {e}")
    
    # åˆ›å»ºæ¨¡æ‹Ÿå™¨
    simulator = AlertSimulator(config)
    
    try:
        # å¯åŠ¨æ¨¡æ‹Ÿ
        simulator.start_simulation()
        
        print(f"Alert simulation running for {args.duration} seconds...")
        print("Press Ctrl+C to stop early")
        
        time.sleep(args.duration)
        
    except KeyboardInterrupt:
        print("\nSimulation interrupted by user")
    finally:
        simulator.stop_simulation()
        
        # å¯¼å‡ºæ•°æ®
        if args.export:
            simulator.export_alerts_to_json(args.export)
        
        # ç”ŸæˆæŠ¥å‘Š
        if args.report:
            report = simulator.generate_alert_report()
            with open(args.report, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            print(f"Alert report saved to {args.report}")
        
        # æ˜¾ç¤ºç®€è¦ç»Ÿè®¡
        report = simulator.generate_alert_report()
        print("\n=== Alert Simulation Summary ===")
        print(f"Active alerts: {report['summary']['active_alerts']}")
        print(f"Alerts in last 24h: {report['summary']['alerts_last_24h']}")
        print(f"Resolution rate: {report['summary']['resolution_rate']:.2%}")
        print(f"Notifications sent: {report['summary']['notifications_sent']}")
        print(f"Notification success rate: {report['summary']['notification_success_rate']:.2%}")


if __name__ == "__main__":
    main()
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Grafanaæ•°æ®ç”Ÿæˆå™¨
ä¸ºGrafanaä»ªè¡¨æ¿ç”Ÿæˆå„ç§æ ¼å¼çš„æµ‹è¯•æ•°æ®
æ”¯æŒInfluxDBã€Prometheusã€Elasticsearchç­‰æ•°æ®æºæ ¼å¼
"""

import argparse
import json
import random
import time
import math
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import os
from enum import Enum
from dataclasses import dataclass

class DataSourceType(Enum):
    """æ•°æ®æºç±»å‹æšä¸¾"""
    INFLUXDB = "influxdb"
    PROMETHEUS = "prometheus"
    ELASTICSEARCH = "elasticsearch"
    MYSQL = "mysql"
    POSTGRES = "postgres"
    GRAPHITE = "graphite"

class MetricType(Enum):
    """æŒ‡æ ‡ç±»å‹æšä¸¾"""
    COUNTER = "counter"
    GAUGE = "gauge"
    HISTOGRAM = "histogram"
    SUMMARY = "summary"

@dataclass
class MetricDefinition:
    """æŒ‡æ ‡å®šä¹‰æ•°æ®ç±»"""
    name: str
    metric_type: MetricType
    description: str
    unit: str
    labels: Dict[str, List[str]]
    value_range: tuple
    pattern: str = "normal"  # normal, seasonal, trending, spiky

class GrafanaDataGenerator:
    """Grafanaæ•°æ®ç”Ÿæˆå™¨"""
    
    def __init__(self, duration: int = 3600, interval: int = 60, data_source: DataSourceType = DataSourceType.INFLUXDB):
        """
        åˆå§‹åŒ–æ•°æ®ç”Ÿæˆå™¨
        
        Args:
            duration: æ•°æ®æ—¶é—´è·¨åº¦(ç§’)
            interval: æ•°æ®ç‚¹é—´éš”(ç§’)
            data_source: æ•°æ®æºç±»å‹
        """
        self.duration = duration
        self.interval = interval
        self.data_source = data_source
        self.start_time = datetime.now() - timedelta(seconds=duration)
        self.end_time = datetime.now()
        self.data_points = []
        
        # å®šä¹‰æŒ‡æ ‡
        self.metrics = {
            # ç³»ç»ŸæŒ‡æ ‡
            'system_cpu_usage': MetricDefinition(
                name='system_cpu_usage',
                metric_type=MetricType.GAUGE,
                description='ç³»ç»ŸCPUä½¿ç”¨ç‡',
                unit='percent',
                labels={
                    'host': ['web-01', 'web-02', 'db-01', 'cache-01'],
                    'cpu': ['cpu0', 'cpu1', 'cpu2', 'cpu3'],
                    'mode': ['user', 'system', 'idle', 'iowait']
                },
                value_range=(0, 100),
                pattern='seasonal'
            ),
            'system_memory_usage': MetricDefinition(
                name='system_memory_usage',
                metric_type=MetricType.GAUGE,
                description='ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡',
                unit='percent',
                labels={
                    'host': ['web-01', 'web-02', 'db-01', 'cache-01'],
                    'type': ['used', 'free', 'cached', 'buffers']
                },
                value_range=(0, 100),
                pattern='trending'
            ),
            'system_disk_usage': MetricDefinition(
                name='system_disk_usage',
                metric_type=MetricType.GAUGE,
                description='ç£ç›˜ä½¿ç”¨ç‡',
                unit='percent',
                labels={
                    'host': ['web-01', 'web-02', 'db-01', 'cache-01'],
                    'device': ['/dev/sda1', '/dev/sda2', '/dev/nvme0n1'],
                    'mountpoint': ['/', '/home', '/var']
                },
                value_range=(0, 100),
                pattern='normal'
            ),
            'system_network_bytes': MetricDefinition(
                name='system_network_bytes',
                metric_type=MetricType.COUNTER,
                description='ç½‘ç»œæµé‡å­—èŠ‚æ•°',
                unit='bytes',
                labels={
                    'host': ['web-01', 'web-02', 'db-01', 'cache-01'],
                    'interface': ['eth0', 'eth1', 'lo'],
                    'direction': ['rx', 'tx']
                },
                value_range=(0, 1000000000),
                pattern='seasonal'
            ),
            
            # åº”ç”¨æŒ‡æ ‡
            'http_requests_total': MetricDefinition(
                name='http_requests_total',
                metric_type=MetricType.COUNTER,
                description='HTTPè¯·æ±‚æ€»æ•°',
                unit='requests',
                labels={
                    'method': ['GET', 'POST', 'PUT', 'DELETE'],
                    'status': ['200', '201', '400', '404', '500'],
                    'endpoint': ['/api/users', '/api/orders', '/api/products', '/health']
                },
                value_range=(0, 10000),
                pattern='seasonal'
            ),
            'http_request_duration': MetricDefinition(
                name='http_request_duration',
                metric_type=MetricType.HISTOGRAM,
                description='HTTPè¯·æ±‚æŒç»­æ—¶é—´',
                unit='seconds',
                labels={
                    'method': ['GET', 'POST', 'PUT', 'DELETE'],
                    'endpoint': ['/api/users', '/api/orders', '/api/products']
                },
                value_range=(0.001, 5.0),
                pattern='spiky'
            ),
            'database_connections': MetricDefinition(
                name='database_connections',
                metric_type=MetricType.GAUGE,
                description='æ•°æ®åº“è¿æ¥æ•°',
                unit='connections',
                labels={
                    'database': ['users_db', 'orders_db', 'products_db'],
                    'state': ['active', 'idle', 'waiting']
                },
                value_range=(0, 200),
                pattern='normal'
            ),
            'database_query_duration': MetricDefinition(
                name='database_query_duration',
                metric_type=MetricType.HISTOGRAM,
                description='æ•°æ®åº“æŸ¥è¯¢æŒç»­æ—¶é—´',
                unit='seconds',
                labels={
                    'database': ['users_db', 'orders_db', 'products_db'],
                    'operation': ['select', 'insert', 'update', 'delete']
                },
                value_range=(0.001, 2.0),
                pattern='spiky'
            ),
            
            # ä¸šåŠ¡æŒ‡æ ‡
            'user_registrations': MetricDefinition(
                name='user_registrations',
                metric_type=MetricType.COUNTER,
                description='ç”¨æˆ·æ³¨å†Œæ•°',
                unit='users',
                labels={
                    'source': ['web', 'mobile', 'api'],
                    'country': ['US', 'CN', 'UK', 'DE', 'JP']
                },
                value_range=(0, 1000),
                pattern='seasonal'
            ),
            'order_amount': MetricDefinition(
                name='order_amount',
                metric_type=MetricType.GAUGE,
                description='è®¢å•é‡‘é¢',
                unit='dollars',
                labels={
                    'currency': ['USD', 'EUR', 'CNY', 'JPY'],
                    'category': ['electronics', 'clothing', 'books', 'food']
                },
                value_range=(10, 5000),
                pattern='seasonal'
            ),
            'active_users': MetricDefinition(
                name='active_users',
                metric_type=MetricType.GAUGE,
                description='æ´»è·ƒç”¨æˆ·æ•°',
                unit='users',
                labels={
                    'platform': ['web', 'ios', 'android'],
                    'region': ['north_america', 'europe', 'asia']
                },
                value_range=(100, 50000),
                pattern='seasonal'
            )
        }
    
    def generate_value(self, metric: MetricDefinition, timestamp: datetime, base_value: float = None) -> float:
        """
        æ ¹æ®æ¨¡å¼ç”ŸæˆæŒ‡æ ‡å€¼
        
        Args:
            metric: æŒ‡æ ‡å®šä¹‰
            timestamp: æ—¶é—´æˆ³
            base_value: åŸºç¡€å€¼
            
        Returns:
            ç”Ÿæˆçš„æŒ‡æ ‡å€¼
        """
        min_val, max_val = metric.value_range
        
        if base_value is None:
            base_value = (min_val + max_val) / 2
        
        if metric.pattern == 'normal':
            # æ­£æ€åˆ†å¸ƒ
            std_dev = (max_val - min_val) * 0.1
            value = random.normalvariate(base_value, std_dev)
            
        elif metric.pattern == 'seasonal':
            # å­£èŠ‚æ€§æ¨¡å¼
            hour = timestamp.hour
            day_of_week = timestamp.weekday()
            
            # å·¥ä½œæ—¥vså‘¨æœ«
            weekend_factor = 0.7 if day_of_week >= 5 else 1.0
            
            # ä¸€å¤©ä¸­çš„æ—¶é—´æ¨¡å¼
            if 6 <= hour <= 9:  # æ—©é«˜å³°
                time_factor = 1.3
            elif 10 <= hour <= 16:  # å·¥ä½œæ—¶é—´
                time_factor = 1.1
            elif 17 <= hour <= 20:  # æ™šé«˜å³°
                time_factor = 1.4
            elif 21 <= hour <= 23:  # æ™šé—´
                time_factor = 0.9
            else:  # å¤œé—´
                time_factor = 0.5
            
            seasonal_factor = weekend_factor * time_factor
            value = base_value * seasonal_factor + random.uniform(-base_value * 0.1, base_value * 0.1)
            
        elif metric.pattern == 'trending':
            # è¶‹åŠ¿æ¨¡å¼
            hours_since_start = (timestamp - self.start_time).total_seconds() / 3600
            trend_factor = 1 + (hours_since_start * 0.02)  # æ¯å°æ—¶å¢é•¿2%
            value = base_value * trend_factor + random.uniform(-base_value * 0.05, base_value * 0.05)
            
        elif metric.pattern == 'spiky':
            # å°–å³°æ¨¡å¼
            if random.random() < 0.05:  # 5%æ¦‚ç‡å‡ºç°å°–å³°
                spike_factor = random.uniform(2, 5)
                value = base_value * spike_factor
            else:
                value = base_value + random.uniform(-base_value * 0.2, base_value * 0.2)
        
        else:
            value = base_value
        
        # ç¡®ä¿å€¼åœ¨èŒƒå›´å†…
        return max(min_val, min(max_val, value))
    
    def generate_label_combinations(self, labels: Dict[str, List[str]], max_combinations: int = 50) -> List[Dict[str, str]]:
        """
        ç”Ÿæˆæ ‡ç­¾ç»„åˆ
        
        Args:
            labels: æ ‡ç­¾å®šä¹‰
            max_combinations: æœ€å¤§ç»„åˆæ•°
            
        Returns:
            æ ‡ç­¾ç»„åˆåˆ—è¡¨
        """
        import itertools
        
        label_keys = list(labels.keys())
        label_values = [labels[key] for key in label_keys]
        
        # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„ç»„åˆ
        all_combinations = list(itertools.product(*label_values))
        
        # å¦‚æœç»„åˆå¤ªå¤šï¼Œéšæœºé€‰æ‹©ä¸€éƒ¨åˆ†
        if len(all_combinations) > max_combinations:
            selected_combinations = random.sample(all_combinations, max_combinations)
        else:
            selected_combinations = all_combinations
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        result = []
        for combination in selected_combinations:
            label_dict = {label_keys[i]: combination[i] for i in range(len(label_keys))}
            result.append(label_dict)
        
        return result
    
    def generate_influxdb_data(self) -> List[Dict[str, Any]]:
        """
        ç”ŸæˆInfluxDBæ ¼å¼çš„æ•°æ®
        
        Returns:
            InfluxDBæ•°æ®ç‚¹åˆ—è¡¨
        """
        data_points = []
        
        current_time = self.start_time
        while current_time <= self.end_time:
            timestamp_ns = int(current_time.timestamp() * 1000000000)
            
            for metric_name, metric in self.metrics.items():
                label_combinations = self.generate_label_combinations(metric.labels)
                
                for labels in label_combinations:
                    value = self.generate_value(metric, current_time)
                    
                    # InfluxDBæ ¼å¼
                    point = {
                        'measurement': metric_name,
                        'tags': labels,
                        'fields': {
                            'value': value
                        },
                        'time': timestamp_ns
                    }
                    
                    data_points.append(point)
            
            current_time += timedelta(seconds=self.interval)
        
        return data_points
    
    def generate_prometheus_data(self) -> List[Dict[str, Any]]:
        """
        ç”ŸæˆPrometheusæ ¼å¼çš„æ•°æ®
        
        Returns:
            Prometheusæ•°æ®ç‚¹åˆ—è¡¨
        """
        data_points = []
        
        current_time = self.start_time
        while current_time <= self.end_time:
            timestamp_ms = int(current_time.timestamp() * 1000)
            
            for metric_name, metric in self.metrics.items():
                label_combinations = self.generate_label_combinations(metric.labels)
                
                for labels in label_combinations:
                    value = self.generate_value(metric, current_time)
                    
                    # Prometheusæ ¼å¼
                    label_str = ','.join([f'{k}="{v}"' for k, v in labels.items()])
                    
                    if metric.metric_type == MetricType.HISTOGRAM:
                        # ç”Ÿæˆç›´æ–¹å›¾æ•°æ®
                        buckets = [0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
                        cumulative_count = 0
                        
                        for bucket in buckets:
                            if value <= bucket:
                                cumulative_count += random.randint(1, 100)
                            
                            point = {
                                'metric': f'{metric_name}_bucket',
                                'labels': {**labels, 'le': str(bucket)},
                                'value': cumulative_count,
                                'timestamp': timestamp_ms
                            }
                            data_points.append(point)
                        
                        # æ·»åŠ _countå’Œ_sumæŒ‡æ ‡
                        data_points.append({
                            'metric': f'{metric_name}_count',
                            'labels': labels,
                            'value': cumulative_count,
                            'timestamp': timestamp_ms
                        })
                        
                        data_points.append({
                            'metric': f'{metric_name}_sum',
                            'labels': labels,
                            'value': value * cumulative_count,
                            'timestamp': timestamp_ms
                        })
                    
                    else:
                        point = {
                            'metric': metric_name,
                            'labels': labels,
                            'value': value,
                            'timestamp': timestamp_ms
                        }
                        data_points.append(point)
            
            current_time += timedelta(seconds=self.interval)
        
        return data_points
    
    def generate_elasticsearch_data(self) -> List[Dict[str, Any]]:
        """
        ç”ŸæˆElasticsearchæ ¼å¼çš„æ•°æ®
        
        Returns:
            Elasticsearchæ–‡æ¡£åˆ—è¡¨
        """
        documents = []
        
        current_time = self.start_time
        while current_time <= self.end_time:
            timestamp_iso = current_time.isoformat()
            
            for metric_name, metric in self.metrics.items():
                label_combinations = self.generate_label_combinations(metric.labels)
                
                for labels in label_combinations:
                    value = self.generate_value(metric, current_time)
                    
                    # Elasticsearchæ–‡æ¡£æ ¼å¼
                    doc = {
                        '@timestamp': timestamp_iso,
                        'metric_name': metric_name,
                        'metric_type': metric.metric_type.value,
                        'value': value,
                        'unit': metric.unit,
                        'description': metric.description,
                        **labels
                    }
                    
                    documents.append(doc)
            
            current_time += timedelta(seconds=self.interval)
        
        return documents
    
    def generate_sql_data(self) -> List[str]:
        """
        ç”ŸæˆSQLæ’å…¥è¯­å¥
        
        Returns:
            SQLè¯­å¥åˆ—è¡¨
        """
        sql_statements = []
        
        # åˆ›å»ºè¡¨ç»“æ„
        create_table_sql = """
CREATE TABLE IF NOT EXISTS metrics (
    id SERIAL PRIMARY KEY,
    timestamp TIMESTAMP NOT NULL,
    metric_name VARCHAR(100) NOT NULL,
    metric_type VARCHAR(20) NOT NULL,
    value DOUBLE PRECISION NOT NULL,
    unit VARCHAR(20),
    labels JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX IF NOT EXISTS idx_metrics_timestamp ON metrics(timestamp);
CREATE INDEX IF NOT EXISTS idx_metrics_name ON metrics(metric_name);
CREATE INDEX IF NOT EXISTS idx_metrics_labels ON metrics USING GIN(labels);
"""
        sql_statements.append(create_table_sql)
        
        current_time = self.start_time
        while current_time <= self.end_time:
            timestamp_str = current_time.strftime('%Y-%m-%d %H:%M:%S')
            
            for metric_name, metric in self.metrics.items():
                label_combinations = self.generate_label_combinations(metric.labels, max_combinations=10)
                
                for labels in label_combinations:
                    value = self.generate_value(metric, current_time)
                    labels_json = json.dumps(labels)
                    
                    insert_sql = f"""
INSERT INTO metrics (timestamp, metric_name, metric_type, value, unit, labels)
VALUES ('{timestamp_str}', '{metric_name}', '{metric.metric_type.value}', {value}, '{metric.unit}', '{labels_json}');
"""
                    sql_statements.append(insert_sql)
            
            current_time += timedelta(seconds=self.interval)
        
        return sql_statements
    
    def generate_dashboard_config(self) -> Dict[str, Any]:
        """
        ç”ŸæˆGrafanaä»ªè¡¨æ¿é…ç½®
        
        Returns:
            ä»ªè¡¨æ¿é…ç½®å­—å…¸
        """
        dashboard = {
            'dashboard': {
                'id': None,
                'title': 'AIOps Test Dashboard',
                'description': 'AIOpsæµ‹è¯•åœºæ™¯ä»ªè¡¨æ¿',
                'tags': ['aiops', 'test', 'monitoring'],
                'timezone': 'browser',
                'refresh': '30s',
                'time': {
                    'from': 'now-1h',
                    'to': 'now'
                },
                'panels': []
            }
        }
        
        panel_id = 1
        y_position = 0
        
        # ç³»ç»ŸæŒ‡æ ‡é¢æ¿
        system_panels = [
            {
                'title': 'CPUä½¿ç”¨ç‡',
                'type': 'graph',
                'targets': [{
                    'expr': 'system_cpu_usage',
                    'legendFormat': '{{host}} - {{cpu}}'
                }],
                'yAxes': [{
                    'unit': 'percent',
                    'max': 100
                }]
            },
            {
                'title': 'å†…å­˜ä½¿ç”¨ç‡',
                'type': 'graph',
                'targets': [{
                    'expr': 'system_memory_usage',
                    'legendFormat': '{{host}} - {{type}}'
                }],
                'yAxes': [{
                    'unit': 'percent',
                    'max': 100
                }]
            },
            {
                'title': 'ç£ç›˜ä½¿ç”¨ç‡',
                'type': 'graph',
                'targets': [{
                    'expr': 'system_disk_usage',
                    'legendFormat': '{{host}} - {{device}}'
                }],
                'yAxes': [{
                    'unit': 'percent',
                    'max': 100
                }]
            },
            {
                'title': 'ç½‘ç»œæµé‡',
                'type': 'graph',
                'targets': [{
                    'expr': 'rate(system_network_bytes[5m])',
                    'legendFormat': '{{host}} - {{interface}} {{direction}}'
                }],
                'yAxes': [{
                    'unit': 'bytes'
                }]
            }
        ]
        
        # åº”ç”¨æŒ‡æ ‡é¢æ¿
        app_panels = [
            {
                'title': 'HTTPè¯·æ±‚ç‡',
                'type': 'graph',
                'targets': [{
                    'expr': 'rate(http_requests_total[5m])',
                    'legendFormat': '{{method}} {{status}}'
                }],
                'yAxes': [{
                    'unit': 'reqps'
                }]
            },
            {
                'title': 'HTTPå“åº”æ—¶é—´',
                'type': 'graph',
                'targets': [{
                    'expr': 'histogram_quantile(0.95, rate(http_request_duration_bucket[5m]))',
                    'legendFormat': '95th percentile'
                }],
                'yAxes': [{
                    'unit': 's'
                }]
            },
            {
                'title': 'æ•°æ®åº“è¿æ¥',
                'type': 'graph',
                'targets': [{
                    'expr': 'database_connections',
                    'legendFormat': '{{database}} - {{state}}'
                }],
                'yAxes': [{
                    'unit': 'short'
                }]
            }
        ]
        
        # ä¸šåŠ¡æŒ‡æ ‡é¢æ¿
        business_panels = [
            {
                'title': 'ç”¨æˆ·æ³¨å†Œ',
                'type': 'stat',
                'targets': [{
                    'expr': 'increase(user_registrations[1h])',
                    'legendFormat': 'æ¯å°æ—¶æ³¨å†Œæ•°'
                }]
            },
            {
                'title': 'è®¢å•é‡‘é¢',
                'type': 'graph',
                'targets': [{
                    'expr': 'order_amount',
                    'legendFormat': '{{currency}} - {{category}}'
                }],
                'yAxes': [{
                    'unit': 'currencyUSD'
                }]
            },
            {
                'title': 'æ´»è·ƒç”¨æˆ·',
                'type': 'graph',
                'targets': [{
                    'expr': 'active_users',
                    'legendFormat': '{{platform}} - {{region}}'
                }],
                'yAxes': [{
                    'unit': 'short'
                }]
            }
        ]
        
        all_panels = system_panels + app_panels + business_panels
        
        for i, panel_config in enumerate(all_panels):
            panel = {
                'id': panel_id,
                'title': panel_config['title'],
                'type': panel_config['type'],
                'gridPos': {
                    'h': 8,
                    'w': 12,
                    'x': (i % 2) * 12,
                    'y': y_position
                },
                'targets': panel_config['targets'],
                'options': {},
                'fieldConfig': {
                    'defaults': {
                        'unit': panel_config.get('yAxes', [{}])[0].get('unit', 'short')
                    }
                }
            }
            
            dashboard['dashboard']['panels'].append(panel)
            panel_id += 1
            
            if i % 2 == 1:
                y_position += 8
        
        return dashboard
    
    def generate_data(self, export_dir: str = "./grafana_data"):
        """
        ç”Ÿæˆæ‰€æœ‰æ ¼å¼çš„æ•°æ®
        
        Args:
            export_dir: å¯¼å‡ºç›®å½•
        """
        print(f"ğŸ“Š å¯åŠ¨Grafanaæ•°æ®ç”Ÿæˆå™¨")
        print(f"æ•°æ®æºç±»å‹: {self.data_source.value}")
        print(f"æ—¶é—´èŒƒå›´: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')} - {self.end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æ•°æ®é—´éš”: {self.interval}ç§’")
        print(f"æŒ‡æ ‡æ•°é‡: {len(self.metrics)}")
        
        os.makedirs(export_dir, exist_ok=True)
        
        # ç”Ÿæˆä¸åŒæ ¼å¼çš„æ•°æ®
        if self.data_source == DataSourceType.INFLUXDB:
            print("ç”ŸæˆInfluxDBæ ¼å¼æ•°æ®...")
            data = self.generate_influxdb_data()
            with open(f"{export_dir}/influxdb_data.json", 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            print(f"InfluxDBæ•°æ®å·²ä¿å­˜: {len(data)}ä¸ªæ•°æ®ç‚¹")
        
        elif self.data_source == DataSourceType.PROMETHEUS:
            print("ç”ŸæˆPrometheusæ ¼å¼æ•°æ®...")
            data = self.generate_prometheus_data()
            with open(f"{export_dir}/prometheus_data.json", 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            print(f"Prometheusæ•°æ®å·²ä¿å­˜: {len(data)}ä¸ªæ•°æ®ç‚¹")
        
        elif self.data_source == DataSourceType.ELASTICSEARCH:
            print("ç”ŸæˆElasticsearchæ ¼å¼æ•°æ®...")
            data = self.generate_elasticsearch_data()
            with open(f"{export_dir}/elasticsearch_data.json", 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            print(f"Elasticsearchæ•°æ®å·²ä¿å­˜: {len(data)}ä¸ªæ–‡æ¡£")
        
        elif self.data_source in [DataSourceType.MYSQL, DataSourceType.POSTGRES]:
            print("ç”ŸæˆSQLæ ¼å¼æ•°æ®...")
            sql_statements = self.generate_sql_data()
            with open(f"{export_dir}/sql_data.sql", 'w', encoding='utf-8') as f:
                f.write('\n'.join(sql_statements))
            print(f"SQLæ•°æ®å·²ä¿å­˜: {len(sql_statements)}æ¡è¯­å¥")
        
        # ç”Ÿæˆä»ªè¡¨æ¿é…ç½®
        print("ç”ŸæˆGrafanaä»ªè¡¨æ¿é…ç½®...")
        dashboard_config = self.generate_dashboard_config()
        with open(f"{export_dir}/dashboard.json", 'w', encoding='utf-8') as f:
            json.dump(dashboard_config, f, indent=2, ensure_ascii=False)
        
        # ç”ŸæˆæŒ‡æ ‡æ–‡æ¡£
        print("ç”ŸæˆæŒ‡æ ‡æ–‡æ¡£...")
        metrics_doc = {
            'metrics': {
                name: {
                    'name': metric.name,
                    'type': metric.metric_type.value,
                    'description': metric.description,
                    'unit': metric.unit,
                    'labels': metric.labels,
                    'value_range': metric.value_range,
                    'pattern': metric.pattern
                }
                for name, metric in self.metrics.items()
            },
            'generation_info': {
                'start_time': self.start_time.isoformat(),
                'end_time': self.end_time.isoformat(),
                'interval': self.interval,
                'data_source': self.data_source.value,
                'total_metrics': len(self.metrics)
            }
        }
        
        with open(f"{export_dir}/metrics_documentation.json", 'w', encoding='utf-8') as f:
            json.dump(metrics_doc, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… æ•°æ®ç”Ÿæˆå®Œæˆ")
        print(f"å¯¼å‡ºç›®å½•: {export_dir}")
        print(f"ç”Ÿæˆæ–‡ä»¶:")
        for file in os.listdir(export_dir):
            file_path = os.path.join(export_dir, file)
            file_size = os.path.getsize(file_path)
            print(f"  - {file} ({file_size:,} bytes)")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Grafanaæ•°æ®ç”Ÿæˆå™¨')
    parser.add_argument('--duration', type=int, default=3600, help='æ•°æ®æ—¶é—´è·¨åº¦(ç§’)')
    parser.add_argument('--interval', type=int, default=60, help='æ•°æ®ç‚¹é—´éš”(ç§’)')
    parser.add_argument('--datasource', type=str, default='influxdb', 
                       choices=['influxdb', 'prometheus', 'elasticsearch', 'mysql', 'postgres'],
                       help='æ•°æ®æºç±»å‹')
    parser.add_argument('--export-dir', type=str, default='./grafana_data', help='å¯¼å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    data_source = DataSourceType(args.datasource)
    generator = GrafanaDataGenerator(
        duration=args.duration,
        interval=args.interval,
        data_source=data_source
    )
    
    generator.generate_data(export_dir=args.export_dir)

if __name__ == '__main__':
    main()
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰©å±•ç»¼åˆæµ‹è¯•è¿è¡Œå™¨

è¿™ä¸ªè„šæœ¬æ‰©å±•äº†åŸæœ‰çš„AIOpsæµ‹è¯•æ¡†æ¶ï¼Œé›†æˆäº†å¤šé¡¹ç›®è´Ÿè½½æµ‹è¯•åŠŸèƒ½ï¼Œ
æ”¯æŒæ›´å…¨é¢çš„ç³»ç»Ÿè´Ÿè½½å’Œé”™è¯¯åœºæ™¯æµ‹è¯•ã€‚

æ–°å¢åŠŸèƒ½ï¼š
- å¤šè¯­è¨€é¡¹ç›®è´Ÿè½½æµ‹è¯• (Java/Rust/Node.js)
- æ„å»ºé”™è¯¯åœºæ™¯æ¨¡æ‹Ÿ
- è¿è¡Œæ—¶é”™è¯¯åœºæ™¯æ¨¡æ‹Ÿ
- å¹¶å‘é¡¹ç›®æµ‹è¯•
- èµ„æºç«äº‰æµ‹è¯•
- é”™è¯¯æ¢å¤æµ‹è¯•

ä½œè€…: AIOpsæµ‹è¯•æ¡†æ¶
åˆ›å»ºæ—¶é—´: 2025-01-10
"""

import os
import sys
import json
import time
import argparse
import subprocess
import threading
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

# å¯¼å…¥åŸæœ‰çš„æµ‹è¯•è¿è¡Œå™¨
try:
    from integrated_test_runner import IntegratedTestRunner
except ImportError:
    print("è­¦å‘Š: æ— æ³•å¯¼å…¥åŸæœ‰æµ‹è¯•è¿è¡Œå™¨ï¼Œå°†ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
    IntegratedTestRunner = None

# å¯¼å…¥å¤šé¡¹ç›®è´Ÿè½½æµ‹è¯•å™¨
try:
    from multi_project_load_tester import ProjectLoadTester
except ImportError:
    print("è­¦å‘Š: æ— æ³•å¯¼å…¥å¤šé¡¹ç›®è´Ÿè½½æµ‹è¯•å™¨")
    ProjectLoadTester = None

class ExtendedTestRunner:
    """æ‰©å±•ç»¼åˆæµ‹è¯•è¿è¡Œå™¨ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ‰©å±•æµ‹è¯•è¿è¡Œå™¨"""
        self.test_results = {}
        self.start_time = None
        self.end_time = None
        self.original_runner = IntegratedTestRunner() if IntegratedTestRunner else None
        self.project_tester = ProjectLoadTester() if ProjectLoadTester else None
        
    def run_command(self, command: str, description: str, timeout: int = 300) -> bool:
        """è¿è¡Œå‘½ä»¤å¹¶è®°å½•ç»“æœ"""
        print(f"\n=== {description} ===")
        print(f"æ‰§è¡Œå‘½ä»¤: {command}")
        
        start_time = time.time()
        try:
            result = subprocess.run(
                command,
                shell=True,
                capture_output=True,
                text=True,
                encoding='utf-8',
                timeout=timeout
            )
            
            end_time = time.time()
            duration = end_time - start_time
            
            success = result.returncode == 0
            
            self.test_results[description] = {
                'command': command,
                'success': success,
                'duration': duration,
                'return_code': result.returncode,
                'stdout': result.stdout,
                'stderr': result.stderr
            }
            
            if success:
                print(f"âœ… {description} å®Œæˆ (è€—æ—¶: {duration:.2f}ç§’)")
            else:
                print(f"âŒ {description} å¤±è´¥ (è€—æ—¶: {duration:.2f}ç§’)")
                print(f"é”™è¯¯ä¿¡æ¯: {result.stderr}")
                
            return success
            
        except subprocess.TimeoutExpired:
            end_time = time.time()
            duration = end_time - start_time
            
            self.test_results[description] = {
                'command': command,
                'success': False,
                'duration': duration,
                'error': f'Command timeout after {timeout} seconds'
            }
            
            print(f"â° {description} è¶…æ—¶ (è€—æ—¶: {duration:.2f}ç§’)")
            return False
            
        except Exception as e:
            end_time = time.time()
            duration = end_time - start_time
            
            self.test_results[description] = {
                'command': command,
                'success': False,
                'duration': duration,
                'error': str(e)
            }
            
            print(f"âŒ {description} å¼‚å¸¸ (è€—æ—¶: {duration:.2f}ç§’)")
            print(f"å¼‚å¸¸ä¿¡æ¯: {e}")
            return False
    
    def run_infrastructure_stress_test(self) -> bool:
        """è¿è¡ŒåŸºç¡€è®¾æ–½å‹åŠ›æµ‹è¯•"""
        print("\nğŸ—ï¸ å¼€å§‹åŸºç¡€è®¾æ–½å‹åŠ›æµ‹è¯•")
        
        # åˆ›å»ºå¤šä¸ªå¹¶å‘çš„åŸºç¡€æµ‹è¯•
        stress_commands = [
            ("python simple_performance_tester.py --test-type cpu --duration 120", "CPUå‹åŠ›æµ‹è¯•"),
            ("python simple_performance_tester.py --test-type memory --duration 120", "å†…å­˜å‹åŠ›æµ‹è¯•"),
            ("python simple_performance_tester.py --test-type disk --duration 120", "ç£ç›˜I/Oå‹åŠ›æµ‹è¯•")
        ]
        
        # å¹¶å‘æ‰§è¡Œå‹åŠ›æµ‹è¯•
        threads = []
        results = {}
        
        def run_stress_command(cmd, desc):
            results[desc] = self.run_command(cmd, desc, timeout=180)
        
        for cmd, desc in stress_commands:
            thread = threading.Thread(target=run_stress_command, args=(cmd, desc))
            threads.append(thread)
            thread.start()
        
        # ç­‰å¾…æ‰€æœ‰å‹åŠ›æµ‹è¯•å®Œæˆ
        for thread in threads:
            thread.join()
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æµ‹è¯•éƒ½æˆåŠŸ
        all_success = all(results.values())
        print(f"\n{'âœ…' if all_success else 'âŒ'} åŸºç¡€è®¾æ–½å‹åŠ›æµ‹è¯•{'å®Œæˆ' if all_success else 'éƒ¨åˆ†å¤±è´¥'}")
        
        return all_success
    
    def run_error_injection_test(self) -> bool:
        """è¿è¡Œé”™è¯¯æ³¨å…¥æµ‹è¯•"""
        print("\nğŸ’¥ å¼€å§‹é”™è¯¯æ³¨å…¥æµ‹è¯•")
        
        if not self.project_tester:
            print("âŒ å¤šé¡¹ç›®è´Ÿè½½æµ‹è¯•å™¨ä¸å¯ç”¨ï¼Œè·³è¿‡é”™è¯¯æ³¨å…¥æµ‹è¯•")
            return False
        
        # é…ç½®é”™è¯¯åœºæ™¯é¡¹ç›®
        error_configs = [
            {'type': 'java', 'name': 'error-prone-service', 'introduce_error': True},
            {'type': 'rust', 'name': 'failing-processor', 'introduce_error': True},
            {'type': 'nodejs', 'name': 'buggy-api', 'introduce_error': True}
        ]
        
        try:
            # è¿è¡Œé”™è¯¯æ³¨å…¥æµ‹è¯•ï¼ˆè¾ƒçŸ­æ—¶é—´ï¼‰
            self.project_tester.run_concurrent_load_test(error_configs, duration=180)
            
            # æ£€æŸ¥æ˜¯å¦äº§ç”Ÿäº†é¢„æœŸçš„é”™è¯¯
            build_failures = self.project_tester.metrics['build_failure']
            runtime_failures = self.project_tester.metrics['runtime_failure']
            
            # é”™è¯¯æ³¨å…¥æµ‹è¯•æˆåŠŸçš„æ ‡å‡†æ˜¯äº§ç”Ÿäº†é”™è¯¯
            success = build_failures > 0 or runtime_failures > 0
            
            print(f"\n{'âœ…' if success else 'âŒ'} é”™è¯¯æ³¨å…¥æµ‹è¯•{'æˆåŠŸ' if success else 'å¤±è´¥'}")
            print(f"æ„å»ºé”™è¯¯: {build_failures}, è¿è¡Œæ—¶é”™è¯¯: {runtime_failures}")
            
            return success
            
        except Exception as e:
            print(f"âŒ é”™è¯¯æ³¨å…¥æµ‹è¯•å¼‚å¸¸: {e}")
            return False
    
    def run_recovery_test(self) -> bool:
        """è¿è¡Œé”™è¯¯æ¢å¤æµ‹è¯•"""
        print("\nğŸ”„ å¼€å§‹é”™è¯¯æ¢å¤æµ‹è¯•")
        
        if not self.project_tester:
            print("âŒ å¤šé¡¹ç›®è´Ÿè½½æµ‹è¯•å™¨ä¸å¯ç”¨ï¼Œè·³è¿‡æ¢å¤æµ‹è¯•")
            return False
        
        # å…ˆè¿è¡Œé”™è¯¯åœºæ™¯
        print("ç¬¬ä¸€é˜¶æ®µ: å¼•å…¥é”™è¯¯")
        error_configs = [
            {'type': 'java', 'name': 'recovery-test-java', 'introduce_error': True},
            {'type': 'nodejs', 'name': 'recovery-test-node', 'introduce_error': True}
        ]
        
        try:
            self.project_tester.run_concurrent_load_test(error_configs, duration=60)
            initial_failures = self.project_tester.metrics['build_failure'] + self.project_tester.metrics['runtime_failure']
            
            # é‡ç½®æŒ‡æ ‡
            self.project_tester.metrics = {
                'build_success': 0,
                'build_failure': 0,
                'runtime_success': 0,
                'runtime_failure': 0,
                'performance_tests': 0
            }
            
            # å†è¿è¡Œæ­£å¸¸åœºæ™¯ï¼ˆæ¨¡æ‹Ÿä¿®å¤åï¼‰
            print("\nç¬¬äºŒé˜¶æ®µ: é”™è¯¯ä¿®å¤åæµ‹è¯•")
            recovery_configs = [
                {'type': 'java', 'name': 'recovery-test-java-fixed', 'introduce_error': False},
                {'type': 'nodejs', 'name': 'recovery-test-node-fixed', 'introduce_error': False}
            ]
            
            self.project_tester.run_concurrent_load_test(recovery_configs, duration=60)
            recovery_successes = self.project_tester.metrics['build_success'] + self.project_tester.metrics['runtime_success']
            
            # æ¢å¤æµ‹è¯•æˆåŠŸçš„æ ‡å‡†æ˜¯å…ˆæœ‰é”™è¯¯ï¼Œåæœ‰æˆåŠŸ
            success = initial_failures > 0 and recovery_successes > 0
            
            print(f"\n{'âœ…' if success else 'âŒ'} é”™è¯¯æ¢å¤æµ‹è¯•{'æˆåŠŸ' if success else 'å¤±è´¥'}")
            print(f"åˆå§‹é”™è¯¯: {initial_failures}, æ¢å¤åæˆåŠŸ: {recovery_successes}")
            
            return success
            
        except Exception as e:
            print(f"âŒ é”™è¯¯æ¢å¤æµ‹è¯•å¼‚å¸¸: {e}")
            return False
    
    def run_comprehensive_load_test(self) -> bool:
        """è¿è¡Œå…¨é¢è´Ÿè½½æµ‹è¯•"""
        print("\nğŸš€ å¼€å§‹å…¨é¢è´Ÿè½½æµ‹è¯•")
        
        if not self.project_tester:
            print("âŒ å¤šé¡¹ç›®è´Ÿè½½æµ‹è¯•å™¨ä¸å¯ç”¨ï¼Œè·³è¿‡å…¨é¢è´Ÿè½½æµ‹è¯•")
            return False
        
        # é…ç½®å…¨é¢æµ‹è¯•åœºæ™¯
        comprehensive_configs = [
            # Javaé¡¹ç›®
            {'type': 'java', 'name': 'microservice-a', 'introduce_error': False},
            {'type': 'java', 'name': 'microservice-b', 'introduce_error': False},
            {'type': 'java', 'name': 'batch-job', 'introduce_error': False},
            
            # Rusté¡¹ç›®
            {'type': 'rust', 'name': 'high-perf-api', 'introduce_error': False},
            {'type': 'rust', 'name': 'data-pipeline', 'introduce_error': False},
            
            # Node.jsé¡¹ç›®
            {'type': 'nodejs', 'name': 'web-frontend', 'introduce_error': False},
            {'type': 'nodejs', 'name': 'websocket-service', 'introduce_error': False},
            {'type': 'nodejs', 'name': 'api-gateway', 'introduce_error': False},
            
            # æ··åˆé”™è¯¯åœºæ™¯
            {'type': 'java', 'name': 'unstable-service', 'introduce_error': True},
            {'type': 'rust', 'name': 'flaky-processor', 'introduce_error': True}
        ]
        
        try:
            # è¿è¡Œå…¨é¢è´Ÿè½½æµ‹è¯•
            self.project_tester.run_concurrent_load_test(comprehensive_configs, duration=300)
            
            # è¯„ä¼°æµ‹è¯•ç»“æœ
            total_tests = sum(self.project_tester.metrics.values())
            successful_tests = self.project_tester.metrics['build_success'] + self.project_tester.metrics['runtime_success']
            
            success_rate = (successful_tests / total_tests * 100) if total_tests > 0 else 0
            success = success_rate >= 70  # 70%æˆåŠŸç‡è§†ä¸ºé€šè¿‡
            
            print(f"\n{'âœ…' if success else 'âŒ'} å…¨é¢è´Ÿè½½æµ‹è¯•{'é€šè¿‡' if success else 'æœªé€šè¿‡'}")
            print(f"æˆåŠŸç‡: {success_rate:.1f}% ({successful_tests}/{total_tests})")
            
            return success
            
        except Exception as e:
            print(f"âŒ å…¨é¢è´Ÿè½½æµ‹è¯•å¼‚å¸¸: {e}")
            return False
    
    def run_original_tests(self) -> bool:
        """è¿è¡ŒåŸæœ‰çš„AIOpsæµ‹è¯•"""
        print("\nğŸ“Š å¼€å§‹åŸæœ‰AIOpsæµ‹è¯•å¥—ä»¶")
        
        if not self.original_runner:
            print("âŒ åŸæœ‰æµ‹è¯•è¿è¡Œå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
            return self._run_simplified_original_tests()
        
        try:
            # è¿è¡ŒåŸæœ‰çš„å¿«é€Ÿæµ‹è¯•
            self.original_runner.run_quick_test()
            
            # æ£€æŸ¥åŸæœ‰æµ‹è¯•ç»“æœ
            if hasattr(self.original_runner, 'test_results'):
                original_results = self.original_runner.test_results
                successful_original = sum(1 for result in original_results.values() if result.get('success', False))
                total_original = len(original_results)
                
                success = successful_original >= total_original * 0.8  # 80%æˆåŠŸç‡
                
                print(f"\n{'âœ…' if success else 'âŒ'} åŸæœ‰AIOpsæµ‹è¯•{'é€šè¿‡' if success else 'æœªé€šè¿‡'}")
                print(f"æˆåŠŸç‡: {successful_original}/{total_original}")
                
                return success
            else:
                print("âœ… åŸæœ‰AIOpsæµ‹è¯•å®Œæˆï¼ˆæ— è¯¦ç»†ç»“æœï¼‰")
                return True
                
        except Exception as e:
            print(f"âŒ åŸæœ‰AIOpsæµ‹è¯•å¼‚å¸¸: {e}")
            return False
    
    def _run_simplified_original_tests(self) -> bool:
        """è¿è¡Œç®€åŒ–ç‰ˆåŸæœ‰æµ‹è¯•"""
        simplified_tests = [
            ("python web_simulator.py --duration 30", "Webåº”ç”¨ç›‘æ§ç®€åŒ–æµ‹è¯•"),
            ("python database_simulator.py --duration 60", "æ•°æ®åº“ç›‘æ§ç®€åŒ–æµ‹è¯•"),
            ("python system_monitor.py --duration 45", "ç³»ç»Ÿç›‘æ§ç®€åŒ–æµ‹è¯•"),
            ("python simple_performance_tester.py --test-type cpu --duration 30", "æ€§èƒ½æµ‹è¯•")
        ]
        
        success_count = 0
        for cmd, desc in simplified_tests:
            if self.run_command(cmd, desc, timeout=120):
                success_count += 1
        
        success = success_count >= len(simplified_tests) * 0.75  # 75%æˆåŠŸç‡
        print(f"\n{'âœ…' if success else 'âŒ'} ç®€åŒ–AIOpsæµ‹è¯•{'é€šè¿‡' if success else 'æœªé€šè¿‡'}")
        print(f"æˆåŠŸç‡: {success_count}/{len(simplified_tests)}")
        
        return success
    
    def run_extended_test_suite(self, test_mode: str = 'comprehensive'):
        """è¿è¡Œæ‰©å±•æµ‹è¯•å¥—ä»¶"""
        print("ğŸ¯ å¼€å§‹æ‰©å±•AIOpsæµ‹è¯•å¥—ä»¶")
        print(f"æµ‹è¯•æ¨¡å¼: {test_mode}")
        print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        self.start_time = time.time()
        
        # å®šä¹‰ä¸åŒæ¨¡å¼çš„æµ‹è¯•åºåˆ—
        if test_mode == 'quick':
            test_sequence = [
                ('åŸæœ‰AIOpsæµ‹è¯•', self.run_original_tests),
                ('åŸºç¡€è®¾æ–½å‹åŠ›æµ‹è¯•', self.run_infrastructure_stress_test)
            ]
        elif test_mode == 'error-focused':
            test_sequence = [
                ('é”™è¯¯æ³¨å…¥æµ‹è¯•', self.run_error_injection_test),
                ('é”™è¯¯æ¢å¤æµ‹è¯•', self.run_recovery_test),
                ('åŸºç¡€è®¾æ–½å‹åŠ›æµ‹è¯•', self.run_infrastructure_stress_test)
            ]
        elif test_mode == 'comprehensive':
            test_sequence = [
                ('åŸæœ‰AIOpsæµ‹è¯•', self.run_original_tests),
                ('åŸºç¡€è®¾æ–½å‹åŠ›æµ‹è¯•', self.run_infrastructure_stress_test),
                ('é”™è¯¯æ³¨å…¥æµ‹è¯•', self.run_error_injection_test),
                ('é”™è¯¯æ¢å¤æµ‹è¯•', self.run_recovery_test),
                ('å…¨é¢è´Ÿè½½æµ‹è¯•', self.run_comprehensive_load_test)
            ]
        else:
            print(f"âŒ æœªçŸ¥æµ‹è¯•æ¨¡å¼: {test_mode}")
            return
        
        # æ‰§è¡Œæµ‹è¯•åºåˆ—
        test_results_summary = {}
        for test_name, test_func in test_sequence:
            print(f"\n{'='*60}")
            print(f"å¼€å§‹æ‰§è¡Œ: {test_name}")
            print(f"{'='*60}")
            
            try:
                result = test_func()
                test_results_summary[test_name] = result
                print(f"\n{'âœ…' if result else 'âŒ'} {test_name} {'å®Œæˆ' if result else 'å¤±è´¥'}")
            except Exception as e:
                test_results_summary[test_name] = False
                print(f"\nâŒ {test_name} å¼‚å¸¸: {e}")
            
            # æµ‹è¯•é—´éš”
            time.sleep(3)
        
        self.end_time = time.time()
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self._generate_extended_test_report(test_results_summary, test_mode)
    
    def _generate_extended_test_report(self, test_results_summary: Dict[str, bool], test_mode: str):
        """ç”Ÿæˆæ‰©å±•æµ‹è¯•æŠ¥å‘Š"""
        total_duration = self.end_time - self.start_time
        successful_suites = sum(1 for result in test_results_summary.values() if result)
        total_suites = len(test_results_summary)
        
        print("\n" + "="*80)
        print("ğŸ¯ æ‰©å±•AIOpsæµ‹è¯•å¥—ä»¶æ‰§è¡Œå®Œæˆ")
        print("="*80)
        print(f"æµ‹è¯•æ¨¡å¼: {test_mode}")
        print(f"æ€»è€—æ—¶: {total_duration:.2f} ç§’")
        print(f"æµ‹è¯•å¥—ä»¶æ€»æ•°: {total_suites}")
        print(f"æˆåŠŸå¥—ä»¶: {successful_suites}")
        print(f"å¤±è´¥å¥—ä»¶: {total_suites - successful_suites}")
        print(f"å¥—ä»¶æˆåŠŸç‡: {(successful_suites/total_suites)*100:.1f}%")
        
        print("\nğŸ“Š æµ‹è¯•å¥—ä»¶è¯¦ç»†ç»“æœ:")
        for suite_name, result in test_results_summary.items():
            status = "âœ…" if result else "âŒ"
            print(f"{status} {suite_name}")
        
        # æ±‡æ€»æ‰€æœ‰æµ‹è¯•ç»“æœ
        all_individual_tests = len(self.test_results)
        successful_individual_tests = sum(1 for result in self.test_results.values() if result.get('success', False))
        
        if all_individual_tests > 0:
            print(f"\nğŸ“ˆ ä¸ªåˆ«æµ‹è¯•ç»Ÿè®¡:")
            print(f"æ€»æµ‹è¯•æ•°: {all_individual_tests}")
            print(f"æˆåŠŸæµ‹è¯•: {successful_individual_tests}")
            print(f"ä¸ªåˆ«æµ‹è¯•æˆåŠŸç‡: {(successful_individual_tests/all_individual_tests)*100:.1f}%")
        
        # é¡¹ç›®è´Ÿè½½æµ‹è¯•ç»Ÿè®¡
        if self.project_tester and hasattr(self.project_tester, 'metrics'):
            metrics = self.project_tester.metrics
            total_project_tests = sum(metrics.values())
            if total_project_tests > 0:
                print(f"\nğŸ—ï¸ é¡¹ç›®è´Ÿè½½æµ‹è¯•ç»Ÿè®¡:")
                print(f"æ„å»ºæˆåŠŸ: {metrics['build_success']}")
                print(f"æ„å»ºå¤±è´¥: {metrics['build_failure']}")
                print(f"è¿è¡ŒæˆåŠŸ: {metrics['runtime_success']}")
                print(f"è¿è¡Œå¤±è´¥: {metrics['runtime_failure']}")
        
        # ä¿å­˜ç»¼åˆæŠ¥å‘Š
        report_data = {
            'test_suite': 'æ‰©å±•AIOpsæµ‹è¯•å¥—ä»¶',
            'test_mode': test_mode,
            'start_time': datetime.fromtimestamp(self.start_time).isoformat(),
            'end_time': datetime.fromtimestamp(self.end_time).isoformat(),
            'total_duration': total_duration,
            'suite_results': test_results_summary,
            'suite_success_rate': (successful_suites/total_suites)*100,
            'individual_test_results': self.test_results,
            'individual_success_rate': (successful_individual_tests/all_individual_tests)*100 if all_individual_tests > 0 else 0,
            'project_load_metrics': self.project_tester.metrics if self.project_tester else None
        }
        
        # åˆ›å»ºæŠ¥å‘Šç›®å½•
        report_dir = Path('extended_test_reports')
        report_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = report_dir / f'extended_aiops_test_{test_mode}_{timestamp}.json'
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        # ç”Ÿæˆæµ‹è¯•å»ºè®®
        self._generate_test_recommendations(test_results_summary, successful_suites, total_suites)
    
    def _generate_test_recommendations(self, test_results: Dict[str, bool], successful_suites: int, total_suites: int):
        """ç”Ÿæˆæµ‹è¯•å»ºè®®"""
        print("\nğŸ’¡ æµ‹è¯•å»ºè®®:")
        
        success_rate = (successful_suites / total_suites) * 100
        
        if success_rate >= 90:
            print("ğŸ‰ ç³»ç»Ÿè¡¨ç°ä¼˜ç§€ï¼æ‰€æœ‰æµ‹è¯•å¥—ä»¶åŸºæœ¬é€šè¿‡ã€‚")
            print("å»ºè®®: å¯ä»¥è€ƒè™‘å¢åŠ æ›´å¤æ‚çš„è´Ÿè½½åœºæ™¯æˆ–æ›´é•¿æ—¶é—´çš„å‹åŠ›æµ‹è¯•ã€‚")
        elif success_rate >= 70:
            print("âœ… ç³»ç»Ÿè¡¨ç°è‰¯å¥½ï¼Œå¤§éƒ¨åˆ†æµ‹è¯•é€šè¿‡ã€‚")
            failed_tests = [name for name, result in test_results.items() if not result]
            if failed_tests:
                print(f"éœ€è¦å…³æ³¨çš„æµ‹è¯•: {', '.join(failed_tests)}")
        elif success_rate >= 50:
            print("âš ï¸ ç³»ç»Ÿå­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œéœ€è¦ä¼˜åŒ–ã€‚")
            failed_tests = [name for name, result in test_results.items() if not result]
            print(f"å¤±è´¥çš„æµ‹è¯•: {', '.join(failed_tests)}")
            print("å»ºè®®: æ£€æŸ¥ç³»ç»Ÿèµ„æºé…ç½®å’Œé”™è¯¯å¤„ç†æœºåˆ¶ã€‚")
        else:
            print("ğŸš¨ ç³»ç»Ÿå­˜åœ¨ä¸¥é‡é—®é¢˜ï¼Œéœ€è¦ç«‹å³å¤„ç†ã€‚")
            print("å»ºè®®: å…¨é¢æ£€æŸ¥ç³»ç»Ÿé…ç½®ã€èµ„æºåˆ†é…å’Œä»£ç è´¨é‡ã€‚")
        
        print("\nğŸ“‹ åç»­è¡ŒåŠ¨é¡¹:")
        print("1. åˆ†æå¤±è´¥æµ‹è¯•çš„è¯¦ç»†æ—¥å¿—")
        print("2. æ£€æŸ¥ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ")
        print("3. ä¼˜åŒ–é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶")
        print("4. è€ƒè™‘å¢åŠ ç›‘æ§å’Œå‘Šè­¦")
        print("5. å®šæœŸè¿è¡Œæµ‹è¯•ä»¥ç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ‰©å±•AIOpsç»¼åˆæµ‹è¯•è¿è¡Œå™¨')
    parser.add_argument(
        '--mode',
        choices=['quick', 'comprehensive', 'error-focused'],
        default='comprehensive',
        help='æµ‹è¯•æ¨¡å¼: quick=å¿«é€Ÿæµ‹è¯•, comprehensive=å…¨é¢æµ‹è¯•, error-focused=é”™è¯¯åœºæ™¯æµ‹è¯•'
    )
    
    args = parser.parse_args()
    
    runner = ExtendedTestRunner()
    runner.run_extended_test_suite(args.mode)

if __name__ == '__main__':
    main()